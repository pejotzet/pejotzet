[{"content":"Komputery jednopłytkowe (ang. SBC — Single Board Computer) to uniwersalne urządzenia mikroprocesorowe pracujące pod kontrolą systemu operacyjnego. Zazwyczaj systemem tym jest Linux. Architektury komputerów jednopłytkowych mogą istotnie się różnić od architektur stosowanych dla komputerów osobistych. Niestety natywne uruchamianie kodu z wykorzystaniem GUI pracującego na systemie target jest często niewykonalne, głównie ze względu na ograniczony interfejs użytkownika i niezbyt duże zasoby urządzenia. Zazwyczaj problem ten rozwiązuje się przy użyciu narzędzi do kompilacji skrośnej.\nW niniejszym opracowaniu przedstawiono podejście alternatywne bazujące na wykorzystaniu emulacji architektury urządzenia docelowego z wykorzystaniem QEMU i połączonej z hermetyzacją środowiska developerskiego w kontenerze Docker\u0026rsquo;a. Dzięki tej technice możliwe jest wygenerowanie kodu binarnego dla urządzenia docelowego bez korzystania z technik kompilacji skrośnej. Jako frontend dewelopera zaproponowano wykorzystanie programu VS Code oraz techniki DevContainers.\nStworzenie środowiska pracy skrośnej jest zadaniem żmudnym, zwłaszcza jeżeli w systemie muszą współegzystować środowiska przeznaczone dla różnych urządzeń docelowych. Oczywiście ideałem byłoby środowisko pracy natywnej na urządzeniu target, jednak opcja ta jest często niemożliwa do realizacji praktycznej ze względu na jego ograniczone zasoby. Okazuje się, że konteneryzacja w połączeniu z emulacją architektury pozwala na stworzenia środowiska budowy aplikacji bardzo zbliżonego tego ideału. Przedstawiona dalej konfiguracja bardzo przypomina pracę zdalną w tunelu SSH [1], jednak tym razem urządzenie docelowe nie jest w zaangażowane w proces budowania aplikacji. Oczywiście jest ono niezbędne do jej debugowania. Proces skrośnego debugowania opisano w podrozdziale \u0026ldquo;Uruchamianie skrośne\u0026rdquo; wspomnianego wcześniej opracowania [1].\nRealizacja przedstawionego dalej scenariusza wymaga zainstalowania na systemie host Docker\u0026lsquo;a i opcjonalnie QEMU. Pozostałe oprogramowanie zostanie umieszczone w kontenerach i może być łatwo usunięte z systemu jednym poleceniem.\nQEMU to dostępne nieodpłatnie środowisko wirtualizacji systemów oraz dynamicznej emulacji architektur. Pierwsza opcja zapewnia możliwość uruchomienia pełnego systemu operacyjnego na wirtualnym urządzeniu. Innymi słowy, wirtualizator zapewnia abstrakcyjną warstwę dostępu do sprzętu. Szczególną cechą QEMU jest możliwość wirtualizacji systemów dla architektur innych niż architektura systemu goszczącego (popularne hypervisory takie jak VirtualBox, VMWare Workstation czy Hyper-V tego nie potrafią). Z kolei dynamiczna emulacja polega na tłumaczeniu w locie kodu binarnego z architektury systemu emulowanego na architekturę systemu goszczącego. Aplikacje uruchamiane w tym trybie korzystają bezpośrednio z jądra systemu goszczącego, w tym sprzętu dostępnego na tym systemie. Z punktu widzenia użytkownika, aplikacje uruchomione w tym trybie wyglądają identycznie jak aplikacje natywne, jedynie wykonują się wolniej ze względu na niezbędną warstwę translacji.\nDocker to środowisko konteneryzacji zapewniające aplikacjom hermetyczne środowisko pracy (dostęp do usług jądra systemu, środowisko sieciowe, system plików), co umożliwia ich bezproblemową dystrybucję i pracę niezależnie od systemu operacyjnego gospodarza. W połączeniu z emulacją architektur zapewnianą przez QEMU możliwe jest uruchamianie i budowanie kontenerów na architekturę inną niż ta, na której uruchomiony jest Docker.\nIdea budowy aplikacji z wykorzystaniem emulacji architektury zakłada\nuruchomienie emulacji architektury systemu target (tego, na którym ma działać aplikacja) na systemie host (tego, na którym zostanie skompilowana),\nprzygotowanie kontenera dla architektury target, który zawiera jego główny system plików uzupełniony o natywne dla target narzędzia budowania aplikacji (kompilator, linker, biblioteki),\nuruchomienie w emulowanym kontenerze kompilacji natywnej (czyli dla architektury target).\nAktywacja emulacji architektury systemu target W systemie host konieczna jest aktywacja emulacji architektury systemu target. Można to osiągnąć, instalując pakiet qemu-user-static\napt install -y qemu-user-static Alternatywnie, można skorzystać z gotowego kontenera\ndocker run --rm --privileged multiarch/qemu-user-static:register --reset Listę emulowanych architektur zwraca komenda\nls /proc/sys/fs/binfmt_misc Przygotowanie kontenera dla architektury systemu target W pustym katalogu należy umieścić archiwum z głównym systemem plików urządzenia target. Archiwum można utworzyć, pobierając, z zachowaniem uprawnień, pliki z karty SD zawierającej firmware urządzenia target (w poniższym przykładzie przyjęto, że po zamontowaniu w systemie host zawartość karty jest widoczna w katalogu \u0026lsquo;/media/akuku/a6a42a97-600f-4ed7-ab06-6a2a169c62f3/\u0026rsquo;). Archiwum \u0026lsquo;rootfs.tar\u0026rsquo; buduje komenda\n(cd /media/akuku/a6a42a97-600f-4ed7-ab06-6a2a169c62f3/ ; sudo tar cvf - *) | cat \u0026gt; ./rootfs.tar Inspekcja zawartości archiwum komendą \u0026rsquo;tar tf rootfs.tar | head -n 10\u0026rsquo; powinna wykazać wynik podobny do poniższego (brak dodatkowych elementów przed \u0026lsquo;bin\u0026rsquo;, \u0026lsquo;boot\u0026rsquo;, itd.)\nbin boot/ boot/boot.cmd boot/orangepi_first_run.txt.template boot/boot.scr boot/orangepiEnv.txt boot/vmlinuz-4.9.170-sun50iw9 boot/config-4.9.170-sun50iw9 boot/uInitrd boot/boot.bmp Zgodnie z zasadą *wszystko można, co nie można, byle z wolna i ostrożna*, główny system plików można również pobrać z działającego urządzenia.\nW tym samym katalogu należy utworzyć plik Dockerfile o treści\nFROM scratch ARG USERNAME=orangepi ARG USER_UID=1000 ARG USER_GID=1000 ADD rootfs.tar / USER root WORKDIR /root RUN apt-get update \u0026amp;\u0026amp; export DEBIAN_FRONTEND=noninteractive \\ \u0026amp;\u0026amp; apt-get -y install build-essential make gpiod ENV HOME=/home/${USERNAME} # [Optional] Customize environment for nonroot user #RUN groupadd --gid \u0026quot;${USER_GID}\u0026quot; \u0026quot;${USERNAME}\u0026quot; \u0026amp;\u0026amp; \\ # useradd --uid \u0026quot;${USER_UID}\u0026quot; --gid \u0026quot;${USER_GID}\u0026quot; --create-home \u0026quot;${USERNAME}\u0026quot; \u0026amp;\u0026amp; \\ # apt-get update \u0026amp;\u0026amp; \\ # apt-get -yq install sudo \u0026amp;\u0026amp; \\ # echo \u0026quot;${USERNAME}\u0026quot; ALL=\\(root\\) NOPASSWD:ALL \u0026gt; \u0026quot;/etc/sudoers.d/${USERNAME}\u0026quot; \u0026amp;\u0026amp; \\ # chmod 0440 \u0026quot;/etc/sudoers.d/${USERNAME}\u0026quot; # \u0026amp;\u0026amp; \\ # usermod -aG docker \u0026quot;${USERNAME}\u0026quot; WORKDIR ${HOME} USER ${USERNAME} CMD uname -a Kompilacja w kontenerze pozostawia pliki binarne w katalogu projektu. Właścicielem tych plików jest użytkownik (określony w kontenerze, tutaj orangepi), w którego imieniu została wykonana kompilacja. Zatem, aby uniknąć kłopotów z dostępem do plików, należy zadbać o to, aby UID i GID użytkownika systemu host i użytkownika w kontenerze były takie same. Wartości te sprawdza się poleceniem id wydanym w terminalu. W powyższym przykładzie przyjęto, że zarówno użytkownik \u0026lsquo;akuku\u0026rsquo; systemu host oraz użytkownik \u0026lsquo;orangepi\u0026rsquo; zdefiniowany w kontenerze mają UID i GID równe 1000.\nPrzedstawiony powyżej plik definiuje sposób budowy kontenera. Przed jej uruchomieniem należy sprawdzić, czy jest ona możliwa\ndocker buildx ls docker buildx use default Jeżeli docelowa platforma jest na liście, to do zbudowania kontenera wystarczy poniższa komenda\ndocker buildx build --platform linux/arm64 -t pzawad/orangepi-emucontainer . wydana w katalogu zawierającym pliki \u0026lsquo;Dockerfile\u0026rsquo; i \u0026lsquo;rootfs.tar\u0026rsquo;. Architekturę zbudowanego kontenera, wersję systemu operacyjnego oraz dostępność kompilatora należy oczywiście sprawdzić\ndocker image inspect pzawad/orangepi-emucontainer | grep Architecture docker run -it --rm pzawad/orangepi-emucontainer bash -c 'uname -a' docker run -it --rm pzawad/orangepi-emucontainer bash -c 'cat /etc/issue' docker run -it --rm pzawad/orangepi-emucontainer bash -c 'gcc -v' Kompilacja emulowana z wykorzystaniem kontenera Kontener z narzędziami dla architektury docelowej pozwala na skompilowanie na systemie host kodu dla systemu target w trybie kompilacji natywnej. Katalog ze źródłami aplikacji jest mapowany do wnętrza kontenera, w którym wywoływany jest kompilator natywny dla platformy docelowej. Kompilator \u0026ldquo;widzi\u0026rdquo; kopię głównego systemu plików urządzenia target, dzięki czemu wszystkie pliki nagłówkowe i biblioteki dynamiczne są na właściwym miejscu. Komenda realizująca kompilację ma np. postać\ndocker run -it --rm --platform=linux/arm64 --name omgthisworks -v ${PWD}:/home/orangepi pzawad/orangepi-emucontainer make Integracja z Visual Studio Code nie nastręcza trudności. Do pliku \u0026lsquo;.vscode/tasks.json\u0026rsquo; wystarczy dodać zadanie postaci\n{ \u0026quot;version\u0026quot;: \u0026quot;2.0.0\u0026quot;, \u0026quot;tasks\u0026quot;: [ { \u0026quot;label\u0026quot;: \u0026quot;Native Build in container\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;shell\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot; docker run -it --rm --platform=linux/arm64 --name omgthisworks -v ${workspaceFolder}:/home/orangepi pzawad/orangepi-emucontainer make\u0026quot;, \u0026quot;group\u0026quot;: { \u0026quot;isDefault\u0026quot;: true, \u0026quot;kind\u0026quot;: \u0026quot;build\u0026quot; }, \u0026quot;options\u0026quot;: { \u0026quot;cwd\u0026quot;: \u0026quot;${workspaceFolder}\u0026quot; }, \u0026quot;problemMatcher\u0026quot;: [ \u0026quot;$gcc\u0026quot; ], \u0026quot;detail\u0026quot;: \u0026quot;This task maps workspace into container and triggers native build inside.\u0026quot; } ] } Podsumowanie Zaletą opisanego podejścia jest prawidłowe dynamiczne łączenie bibliotek bez żadnych dodatkowych zabiegów. Co więcej, nie jest konieczna instalacja narzędzi do kompilacji ani na systemie target, ani na systemie host. Kontener służący do tworzenia aplikacji może być łatwo dystrybuowany pomiędzy programistami. W efekcie procedurę przygotowania kontenera zawierającego oprogramowanie niezbędne do pracy nad aplikacja wystarczy przeprowadzić tylko raz. Ma to olbrzymie znaczenie w pracy grupowej gdzie hermetyczność narzędzi i powtarzalność procedur mają ogromne znaczenie.\nBibliografia [1] PeJotZet, \u0026ldquo; Jak kompilować i debugować aplikacje w C/C\u0026#43;\u0026#43; na komputery jednopłytkowe korzystając z VS Code \u0026rdquo;, 2023\nKruczki i sztuczki Zdalny dostęp do konta administratora bez podawania hasła Zdalny dostęp do konta administratora\nW pliku /etc/ssh/sshd_config\u0026rsquo; (target) należy ustawić opcję\nPermitRootLogin yes Logowanie bez konieczności podawania hasła\nSerwer ssh musi zezwalać na uwierzytelnienie za pomocą kluczy publicznych. W pliku /etc/ssh/sshd_config\u0026rsquo; (target) należy ustawić opcje\nPubkeyAuthentication yes AuthorizedKeysFile .ssh/authorized_keys .ssh/authorized_keys2 Użytkownik systemu host musi mieć wygenerowane osobiste klucze ssh. Generacja komendą\nssh-keygen Opcją \u0026lsquo;-t\u0026rsquo; można wymusić typ klucza, a opcją \u0026lsquo;-b\u0026rsquo; jego długość. Wygenerowane klucze, prywatny i publiczny, umieszczone są w podkatalogu \u0026lsquo;.ssh\u0026rsquo; katalogu domowego (publiczny ma rozszerzenie \u0026lsquo;pub\u0026rsquo;).\nKlucz publiczny należy umieścić na systemie target w pliku \u0026lsquo;.ssh/authorized_keys\u0026rsquo; na koncie docelowym. Wygodne kopiowanie zapewnia komenda (poniżej użyto konta root)\nssh-copy-id root@opi.usb Pobranie głównego systemu plików z działającego urządzenia Jeżeli główny system plików archiwizujemy z działającego urządzenia target, to przed operacją archiwizacji katalog główny należy zamontować w innym miejscu przy użyciu \u0026lsquo;mount \u0026ndash;bind\u0026rsquo; i dopiero w nim wydać komendę archiwizacji. Wynik archiwizacji najlepiej od razu pobrać na system host\nssh root@opi.usb 'mkdir -p /tmp/rootfs \u0026amp;\u0026amp; mount --bind / /tmp/rootfs \u0026amp;\u0026amp; cd /tmp/rootfs/ \u0026amp;\u0026amp; tar cf - *' \u0026gt; rootfs.tar W powyższej komendzie opi.usb to nazwa sieciowa urządzenia target i może być zastąpiona przez jego numer IP.\n","date":"2023-08-13","permalink":"https://pejotzet.github.io/pejotzet/post/2023-08-13-emulated-building-for-sbc/","tags":["C/C++","SBC","Docker","DevContainers","QEMU"],"title":"Budowanie aplikacji dla SBC w środowisku emulowanym"},{"content":"Komputery jednopłytkowe (ang. SBC — Single Board Computer) to uniwersalne urządzenia mikroprocesorowe pracujące pod kontrolą systemu operacyjnego. Zazwyczaj systemem tym jest Linux. Architektury komputerów jednopłytkowych mogą istotnie się różnić od architektur stosowanych dla komputerów osobistych. W rezultacie przygotowanie aplikacji w postaci binarnej na urządzenie docelowe (target) na komputerze osobistym (host) wymaga wielu, często dość skomplikowanych, zabiegów. Uwaga niniejszego opracowania skupiona jest na realizacji różnych scenariuszy pracy z wykorzystaniem edytora VS Code i eliminujących konieczność instalacji i konfiguracji środowiska graficznego na SBC.\nProducenci urządzeń klasy SBC nieustannie przekonują potencjalnych nabywców, że urządzenia te mogą funkcjonować jako ubogie wersje komputerów osobistych, jednak nie znam nikogo, kto używałby ich w ten sposób. Tak naprawdę siłą komputerów tego typu jest dostępność niskopoziomowych interfejsów takich, jak linie GPIO czy magistrale I2C, SPI przy jednoczesnej dostępności wielu serwisów sieciowych. Jądro systemu operacyjnego zainstalowanego na takim urządzeniu zapewnia abstrakcyjny interfejs dostępu sprzętu, dzięki czemu aplikacje wyglądają w zasadzie tak samo, niezależnie od platformy sprzętowej wykorzystanej do realizacji urządzenia SBC.\nIdealną sytuacją byłoby, gdyby aplikacje dla komputera jednopłytkowego można było tworzyć, budować i uruchamiać bezpośrednio na nim. Komfortowa praca wymaga jednak uruchomienia konsoli graficznej (instalacji oprogramowania oraz doposażenia w dodatkowy sprzęt w postaci myszy, klawiatury i monitora), zapewnienia odpowiedniej przestrzeni dyskowej i wystarczającego rozmiaru pamięci RAM. Scenariusz taki jest możliwy, gdy SBC ma zasoby porównywalne z komputerami PC. W zdecydowanej większości przypadków tak nie jest.\nWe wszystkich opisanych dalej scenariuszach jedynym interfejsem programisty będzie Visual Studio Code \u0026ndash; doceniony przez programistów i wysoce konfigurowalny edytor. Mechanizm rozszerzeń w edytorze umożliwia jego uzupełnienie o funkcje budowania i uruchamiania aplikacji co de facto przekształca go w zintegrowane środowisko pracy. VS Code jest dostępny nieodpłatnie na wszystkie popularne systemy operacyjne. Pracę z projektami C/C++ znacznie ułatwia *C/C++ Extension Pack*. Dalej przyjęto, że rozszerzenie to jest zainstalowane. Rozważane są scenariusze pracy w których kod aplikacji jest budowany na urządzeniu docelowym lub komputerze PC.\n1. Kompilacja natywna Opisane w tym punkcie scenariusze wymagają instalacji w systemie target narzędzi umożliwiających budowanie aplikacji, np.\napt install -y build-essentials make apt install -y libgpiod-dev VS Code jest wykorzystywane jedynie do zapewnienia programiście wygodnego interfejsu graficznego.\n1.1 Praca zdalna z wykorzystaniem Code Server Instalacja w Visual Studio Code rozszerzenia Remote SSH umożliwia pracę zdalną na dowolnym koncie SSH, tak jak gdyby był to komputer lokalny. W procesie automatycznej konfiguracji zdalnego konta na systemie target jest instalowany i uruchamiany Code Server. Otwarcie zdalnej sesji przebiega następująco:\npo uruchomieniu Visual Studio Code wystarczy wybrać zielony przycisk \u0026ldquo; \u0026rdquo; w lewym dolnym rogu ekranu\nz palety komend wybrać \u0026ldquo;Connect to Host\u0026rdquo;,\nw polu adresu wpisać root@opi.usb (\u0026ldquo;root\u0026rdquo; można zamienić na dowolne inne konto, \u0026ldquo;opi.usb\u0026rdquo; jest nazwą sieciową systemu target i może być zamieniona na stosowny numer IP),\nRozszerzenie Remote SSH automatycznie zainstaluje na zdalnym koncie oprogramowanie Code Server (w katalogu \u0026lsquo;.vscode\u0026rsquo; na zdalnym koncie) umożliwiające egzekucję i odbieranie wyników komend wydawanych w tunelu SSH. Praca w takiej konfiguracji jest bardzo wygodna i w zasadzie niczym nie różni się od pracy lokalnej. Jedyna różnica polega na tym, że źródła aplikacji i narzędzia ich kompilacji muszą być obecne na systemie target. Na systemie host nie trzeba instalować nić oprócz Visual Studio Code. Niestety, przedstawione rozwiązanie ma dwie (poważne) wady:\ncode-server wspiera tylko architektury 64-bitowe: amd64 i arm64, co na wyklucza zastosowanie tego podejścia na wielu dostępnych na rynku SBC,\nsystem target powinien być wyposażony w 1GB RAM i dwurdzeniowy CPU.\n1.2 Praca zdalna w tunelu SSH Poprzednia metoda nie jest uniwersalna ze względu na wymagania stawiane przez Code Server. Element ten można wyeliminować z poprzedniego schematu. Tym razem pliki źródłowe są edytowane na systemie host i przed kompilacją kopiowane na system target, gdzie są kompilowane za pomocą natywnych narzędzi. W toku dalszego wywodu zostanie przyjęto następującą strukturę podkatalogów w katalogu projektu\n.vscode/ -\u0026gt; katalog z plikami konfiguracyjnymi IDE obj/ -\u0026gt; katalog pomocniczy dla make src/ -\u0026gt; pliki źródłowe projektu src/include/ src/main.c Makefile Powyższy projekt można zbudować natywnie, korzystając z następującego \u0026lsquo;Makefile\u0026rsquo;\n# Compiler settings - Can be customized. CC = gcc CXX = g++ CPPFLAGS = CFLAGS = -std=c11 -g -Wall -fdiagnostics-color=always CXXFLAGS = -std=c++11 -g -Wall -fdiagnostics-color=always LDFLAGS = -lpthread -lgpiod # Makefile settings - Can be customized. APPNAME = main EXT = .c SRCDIR = src OBJDIR = obj REMOTE = root@opi.usb REMOTEPWD = /root/RemotePowerButton SRC = $(wildcard $(SRCDIR)/*$(EXT)) OBJ = $(SRC:$(SRCDIR)/%$(EXT)=$(OBJDIR)/%.o) DEP = $(OBJ:$(OBJDIR)/%.o=%.d) RM = rm all: $(APPNAME) # Copy sources to target copysrc: scp -p -r ./src/ ./obj/ ./Makefile $(REMOTE):$(REMOTEPWD) # Builds the app $(APPNAME): $(OBJ) $(CC) $(CFLAGS) -o $@ $^ $(LDFLAGS) # Creates the dependecy rules %.d: $(SRCDIR)/%$(EXT) @$(CPP) $(CFLAGS) $(CPPFLAGS) $\u0026lt; -MM -MT $(@:%.d=$(OBJDIR)/%.o) \u0026gt;$@ # Includes all .h files -include $(DEP) # Building rule for .o files and its .c/.cpp in combination with all .h $(OBJDIR)/%.o: $(SRCDIR)/%$(EXT) $(CC) $(CFLAGS) $(CPPFLAGS) -o $@ -c $\u0026lt; # Cleans complete project .PHONY: clean clean: -$(RM) $(OBJ) $(DEP) $(APPNAME) # Cleans only all files with the extension .d .PHONY: cleandep cleandep: -$(RM) $(DEP) W powyższym pliku cel \u0026lsquo;copysrc\u0026rsquo; odpowiada za skopiowanie na system target struktury katalogów i plików niezbędnych do zbudowania aplikacji.\nPlikiem konfiguracyjnym mówiącym Visual Studio Code jak należy dany projekt budować, jest plik \u0026lsquo;.vscode/tasks.json\u0026rsquo;. Każdy projekt może mieć kilka zadań, do których odwołujemy się przez nazwę nadaną w polu \u0026lsquo;label\u0026rsquo;. Zadania budowania konfiguruje się w menu \u0026lsquo;Terminal\u0026rsquo;. Konfiguracja zadania, które kopiuje pliki źródłowe na system target, a następnie zdalnie wykonuje na nim polecenie \u0026lsquo;make\u0026rsquo; wygląda następująco (plik \u0026lsquo;.vscode/tasks.json\u0026rsquo;):\n{ \u0026quot;version\u0026quot;: \u0026quot;2.0.0\u0026quot;, \u0026quot;tasks\u0026quot;: [ { \u0026quot;label\u0026quot;: \u0026quot;Native Build on Target\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;shell\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;make copysrc ; ssh root@opi.usb '(cd /root/RemotePowerButton; make)'\u0026quot;, \u0026quot;group\u0026quot;: { \u0026quot;kind\u0026quot;: \u0026quot;build\u0026quot;, \u0026quot;isDefault\u0026quot;: true }, \u0026quot;options\u0026quot;: { \u0026quot;cwd\u0026quot;: \u0026quot;${workspaceFolder}\u0026quot; }, \u0026quot;problemMatcher\u0026quot;: [ \u0026quot;$gcc\u0026quot; ], \u0026quot;detail\u0026quot;: \u0026quot;This task copies project sources to target and then it builds them natively there.\u0026quot; } ] } Przyjęto, że zdalnym katalogiem projektu jest \u0026lsquo;/root/RemotePowerButton\u0026rsquo;. Procedurę budowania aktywuje się skrótem klawiszowym \u0026lsquo;Ctrl+Shift+B\u0026rsquo;.\nEdytor Visual Studio Code jest wyposażony w mechanizm IntelliSense bardzo ułatwiający pisanie kodu źródłowego poprzez zastosowanie zaawansowanego mechanizmu podpowiedzi nt. zdefiniowanych zmiennych, dostępnych funkcji, akceptowanych przez nie argumentów itp. IntelliSense czerpie swoją wiedzę z analizy treści plików nagłówkowych. W przedstawionej tutaj konfiguracji problem polega na tym, że IntelliSense korzysta z plików nagłówkowych w systemie host, które mogą być zupełnie odmienne od odpowiednich plików w systemie target. Aby tę niefortunny stan rzeczy naprawić, należy pliki nagłówkowe z systemu target udostępnić IntelliSense. Procedura jest dwuetapowa:\npliki nagłówkowe z systemu target, muszą być widoczne w systemie host,\nIntelliSense musi być poinformowane, gdzie się te pliki znajdują.\nPunkt 1. można zrealizować na dwa sposoby:\nmetodą \u0026ldquo;brutalnej siły\u0026rdquo; polegającej na skopiowaniu rzeczonych plików nagłówkowych na system host (przeniesienie atrybutu właściciela pliku nie jest konieczne),\n# w katalogu projektu na systemie host mkdir -p rootfs/usr/include scp -r root@opi.usb:/usr/include rootfs/usr/include/ metodą \u0026ldquo;elegancką\u0026rdquo; polegającą na udostępnieniu w trybie tylko do odczytu odpowiedniego fragmentu systemu plików systemu target z wykorzystaniem protokołu NFS.\nMetoda \u0026ldquo;brutalnej siły\u0026rdquo; ma tę wadę, że przynajmniej co do zasady, powinna być powtórzona po każdorazowej aktualizacji oprogramowania na systemie target. Metodę elegancką opisano w punkcie \u0026ldquo;Kruczki i sztuczki\u0026rdquo;.\nNastępnie, korzystając z pliku \u0026lsquo;.vscode/c_cpp_properties.json\u0026rsquo; należy poinformować IntelliSense, które pliki powinny być analizowane\n{ \u0026quot;configurations\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;Target Rootfs\u0026quot;, \u0026quot;includePath\u0026quot;: [ \u0026quot;${workspaceFolder}/src/**\u0026quot;, \u0026quot;${workspaceFolder}/rootfs/**\u0026quot; ], \u0026quot;defines\u0026quot;: [ // symbols resolved by the preprocessor that are unknown to IntelliSense \u0026quot;NULL=0\u0026quot; // I suspect that \u0026quot;# define NULL 0\u0026quot; in types.h is accepted by cpp, but IGNORED by IntelliSense ], \u0026quot;compilerPath\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;cStandard\u0026quot;: \u0026quot;c11\u0026quot;, \u0026quot;cppStandard\u0026quot;: \u0026quot;c++11\u0026quot;, \u0026quot;intelliSenseMode\u0026quot;: \u0026quot;linux-gcc-arm64\u0026quot; } ], \u0026quot;version\u0026quot;: 4 } Zagadnienie udostępnienia właściwych plików nagłówkowych będzie się przewijało we wszystkich opisywanych dalej konfiguracjach. W wersji korzystającej z plików udostępnionych za pomocą NFS katalog \u0026lsquo;rootfs\u0026rsquo; może być po prostu linkiem symbolicznym do katalogu, w którym widoczne są pliki urządzenia target.\n2. Kompilacja skrośna Proces budowania aplikacji złożony jest dwóch zasadniczych etapów:\nkompilacji, czyli przekształcenia kodu źródłowego na kod binarny dla docelowej architektury,\nłączenia (ang. liniking), czyli zebrania wielu fragmentów kodu binarnego w jedną aplikację.\nKompilacji podlega kod źródłowy stworzony przez twórcę oprogramowania wraz z kodem zawartym w plikach nagłówkowych bibliotek. Łączeniu podlega kod binarny wyprodukowany w procesie kompilacji z kodem binarnym bibliotek. Kompilator czerpie z plików nagłówkowych informację o kodzie, który będzie dostępny na etapie łączenia, zatem oba te elementy muszą pozostawać w pełnym synchronizmie. Sam proces łączenia ma dwie odmiany. W łączeniu statycznym wymuszanym na linkerze opcją -static kod binarny aplikacji oraz wspomagający go kod z bibliotek są \u0026lsquo;sklejane\u0026rsquo; w jeden plik. Tak zbudowana aplikacja jest samowystarczalna, jednak jej kod wynikowy może być bardzo duży (np. użycie funkcji printf implikuje dołączenie znacznego fragmentu standardowej biblioteki C). Bez opcji -static linker pracuje w trybie łączenia dynamicznego, w którym kod binarny aplikacji jest uzupełniany informacją o nazwie i wersji biblioteki, o którą powinien być uzupełniony w momencie uruchomienia. Dzięki takiemu podejściu kod wynikowy aplikacji jest stosunkowo mały, a biblioteki wykorzystywane przez wiele aplikacji są ładowane do pamięci tylko raz. Niestety, w momencie uruchomienia aplikacja nie jest samodzielna jak w przypadku łączenia statycznego, gdyż system, w którym jest uruchamiana, musi być uzupełniony o biblioteki współdzielone niezbędne do działania aplikacji (z dokładnością do wersji biblioteki, aby zapewnić zgodność interfejsu biblioteki zakładanego podczas kompilacji). Ostatecznie budowanie skrośne wymaga spełnienia następujących warunków:\nkompilator musi wytwarzać kod binarny zgodny z architekturą urządzenia target,\nprzetwarzane przez kompilator pliki nagłówkowe muszą być zgodne z kodem binarnym łączonym przez linker,\nna etapie uruchamiania, dla programów łączonych dynamicznie, w systemie target muszą być zainstalowane dokładnie te same biblioteki, które były przyjęte na etapie łączenia. Mówiąc krótko, architektura kodu, deklaracje zawarte w kodach nagłówkowych i biblioteki podczas łączenia i uruchamiania aplikacji muszą do siebie pasować.\nPodstawowym problemem skrośnego tworzenia oprogramowania jest konstrukcja środowiska gwarantującego spełnienie warunków 1-3. Ich poprawne spełnienie wymaga istotnej ingerencji w oprogramowanie zainstalowane na systemie host. Zestaw narzędzi umożliwiających przygotowanie kodu binarnego na architekturę inną niż architektura systemu na którym wykonywania jest kompilacja nazywa się toolchain\u0026lsquo;em. Nazwa ta podkreśla fakt, że skrośne budowanie aplikacji wymaga zharmonizowanego użycia wielu narzędzi.\nSkrośne budowanie aplikacji jest żmudne i podatne na błędy\n2.1 Instalacja kompilatora skrośnego w systemie host Budowanie skrośne jest stosunkowo proste, gdy dla systemu host istnieje gotowy pakiet zawierający zestaw narzędzi do skrośnego budowania aplikacji (tzw. toolchain) oraz podstawowe biblioteki dla architektury docelowej\napt list \u0026quot;crossbuild*\u0026quot; Jego instalacja umożliwia bezproblemową kompilację prostych aplikacji. Problem koegzystencji został rozwiązany poprzez zastosowanie konwencji nazewniczej polegającej na poprzedzeniu każdego programu wchodzącego w skład toolchainu prefiksem opisującym platformę docelową w formacie (łącznik na końcu prefiksu jest jego integralną częścią)\n\u0026lt;arch\u0026gt;-\u0026lt;os\u0026gt;-\u0026lt;lib\u0026gt;- \u0026lt;arch\u0026gt;-\u0026lt;vendor\u0026gt;-\u0026lt;os\u0026gt;-\u0026lt;lib\u0026gt;- Na przykład typowymi prefiksami dla architektury ARM64 są aarch64-linux-gnu- lub aarch64-unknown-linux-gnu-\nŁączenie statyczne Szansa, że wersje bibliotek zainstalowanych w \u0026lsquo;crossbuild\u0026rsquo; pokrywają się z bibliotekami w środowisku uruchomiania, jest niewielka, zatem konieczne jest uwolnienie się od warunku 3, poprzez wymuszenie łączenia statycznego np.\nPREFIX=aarch64-linux-gnu- ; ${PREFIX}gcc -c -g -Wall -o main.o main.c ; ${PREFIX}gcc -static main.o Kompilator i linker skrośny \u0026lsquo;wiedzą\u0026rsquo;, gdzie w systemie host zainstalowano pliki stosowne dla architektury docelowej, co znacznie upraszcza składnię komend niezbędnych do skutecznego zbudowania aplikacji. Aplikacja zbudowana jak wyżej może być uruchomiona na dowolnym systemie zgodnym z architekturą zdefiniowaną w zmiennej PREFIX. Jednak za prostotę płacimy cenę: po pierwsze, aplikacja jest stosunkowo duża i po drugie, uzupełnienie zależności o bibliotekę, której brak w pakiecie \u0026lsquo;crossbuild*\u0026rsquo; wymaga ręcznej kompilacji skrośnej biblioteki i ujęcia jej explicite w wywołaniach kompilatora i linkera.\nŁączenie dynamiczne Budowanie skrośne wykorzystujące łączenie dynamiczne jest bardziej złożone, gdyż na systemie host muszą znajdować te same biblioteki dynamiczne/współdzielone co na systemie target. Spełnienie punktu 3 implikuje również instalację plików nagłówkowych skojarzonych z tymi bibliotekami. Katalog zawierający te elementy jest nazywany sysroot. Zarówno kompilator skrośny, jak i linker skrośny muszą być poinformowane o jego położeniu. Zawartość sysroot ekstrahuje się z głównego systemu plików systemu target. Wymóg utrzymania synchronizmu wymusza aktualizację jego kopii na systemie host po zmianie firmware na urządzeniu target.\nPREFIX=aarch64-linux-gnu- ; SYS=rootfs ;${PREFIX}gcc --sysroot ${SYS} -c -g -Wall -o main.o main.c ; ${PREFIX}gcc --sysroot ${SYS} main.o Wartość zmiennej PREFIX jest używana do rozmieszczania plików w drzewie katalogów na etapie przygotowywania rootfs. Jednocześnie jest ona wykorzystywana przez kompilator skrośny do poszukiwania plików. Zatem łączenie dynamiczne aby prefiks kompilatora był taki sam jak dla rootfs.\nSpełnienie tego warunku jest konieczne do bezproblemowej kompilacji i łączenia w trybie dynamicznym\nInną, bardziej elegancką metodą zapewnienia widoczności plików z urządzenia target jest ich udostępnienie z wykorzystaniem protokołu NFS.\nPoniżej przedstawiono definicje zadań kompilacji statycznej i dynamicznej w Visual Studio Code. Dla kompilacji dynamicznej przyjęto założenie, że system plików urządzenia target jest widoczny w katalogu \u0026lsquo;rootfs/\u0026rsquo;.\n{ \u0026quot;version\u0026quot;: \u0026quot;2.0.0\u0026quot;, \u0026quot;tasks\u0026quot;: [ { \u0026quot;label\u0026quot;: \u0026quot;CrossBuild with static linkage\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;shell\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;make PREFIX=aarch64-linux-gnu- STATIC=-static\u0026quot;, \u0026quot;group\u0026quot;: { //\u0026quot;isDefault\u0026quot;: true, \u0026quot;kind\u0026quot;: \u0026quot;build\u0026quot; }, \u0026quot;options\u0026quot;: { \u0026quot;cwd\u0026quot;: \u0026quot;${workspaceFolder}\u0026quot; }, \u0026quot;problemMatcher\u0026quot;: [ \u0026quot;$gcc\u0026quot; ], \u0026quot;detail\u0026quot;: \u0026quot;This task creates static application produced with cross compiler installed on host.\u0026quot; }, { \u0026quot;label\u0026quot;: \u0026quot;CrossBuild with dynamic linkage\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;shell\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;make PREFIX=aarch64-linux-gnu- SYSROOT=rootfs/\u0026quot;, \u0026quot;group\u0026quot;: { //\u0026quot;isDefault\u0026quot;: true, \u0026quot;kind\u0026quot;: \u0026quot;build\u0026quot; }, \u0026quot;options\u0026quot;: { \u0026quot;cwd\u0026quot;: \u0026quot;${workspaceFolder}\u0026quot; }, \u0026quot;problemMatcher\u0026quot;: [ \u0026quot;$gcc\u0026quot; ], \u0026quot;detail\u0026quot;: \u0026quot;This task creates dynamically linked application produced with cross compiler installed on host. Target's rootfs is visible in rootfs/ folder.\u0026quot; } ] } 2.2 Konteneryzacja środowiska kompilacji skrośnej Instalowanie w systemie host kilku środowisk skrośnych prowadzi do dość sporego zamieszania w organizacji plików, zwłaszcza jeżeli narzędzia te pochodzą z równych źródeł. Atrakcyjną alternatywą jest zamknięcie tych środowisk w kontenerach Docker\u0026lsquo;a. Zaletą jest o wiele łatwiejsze zarządzanie i całkowity brak interferencji pomiędzy tak zorganizowanymi środowiskami. W ramach projektu Dockcross zamknięto w kontenerach narzędzia kompilacji na wiele platform docelowych. Ich zawartość obejmuje narzędzia do budowania aplikacji oraz standardową bibliotekę C (zrealizowaną jako GNU C, musl lub uClib). Korzystanie z kontenerów jest wyjątkowo proste: instalacja sprowadza się do jednej komendy, a budowanie aplikacji wymagaja kosmetycznego retuszu pliku Makefile. Dostępne typy architektur można sprawdzić w repozytorium GitHub projektu https://github.com/dockcross/dockcross lub na https://hub.docker.com/.\nInstalację środowiska realizuje komenda\ndocker run --rm dockcross/arch-name \u0026gt; ./dockcross chmod +x ./dockcross W jej wyniku na system host zostanie pobrany odpowiedni kontener, a w katalogu projektu utworzony skrypt dockcross umożliwiający korzystanie z jego zawartości. Jeżeli kontener ma być używany również w innych projektach to skrypt należy przenieść do katalogu znajdującego się na ścieżce dostępu i zmienić jego nazwę na taką, która informuje o nazwie architektury. Parametry linii komend tego skryptu traktowane są jak linia poleceń, którą należy przekazać do kontenera. Jednocześnie bieżący katalog jest mapowany do wnętrza kontenera, co powoduje, że pliki projektu są dostępne dla kompilatora w katalogu roboczym kontenera. Na przykład, aby sprawdzić jak nazywa się kompilator umieszczony w kontenerze wystarczy wydać komendę\n./dockcross bash -c 'echo $CC' Powyższa komenda pozwala na ustalenie wartości zmiennej PREFIX używanej we wcześniejszych przykładach. Na przykład dla kontenera dockcross/linux-arm64 przyjmie ona wartość aarch64-unknown-linux-gnu-. W rezultacie komenda realizująca kompilację skrośną wymaga prostego uzupełnienia o wywołanie skryptu\n./dockcross bash -c 'PREFIX=aarch64-unknown-linux-gnu- ; ${PREFIX}gcc -c -g -Wall -o main.o main.c ; ${PREFIX}gcc -static main.o -o myapp' { \u0026quot;version\u0026quot;: \u0026quot;2.0.0\u0026quot;, \u0026quot;tasks\u0026quot;: [ { \u0026quot;label\u0026quot;: \u0026quot;Containerized CrossBuild with static linkage\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;shell\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;./dockcross bash -c 'make PREFIX=aarch64-unknown-linux-gnu- STATIC=-static'\u0026quot;, \u0026quot;group\u0026quot;: { //\u0026quot;isDefault\u0026quot;: true, \u0026quot;kind\u0026quot;: \u0026quot;build\u0026quot; }, \u0026quot;options\u0026quot;: { \u0026quot;cwd\u0026quot;: \u0026quot;${workspaceFolder}\u0026quot; }, \u0026quot;problemMatcher\u0026quot;: [ \u0026quot;$gcc\u0026quot; ], \u0026quot;detail\u0026quot;: \u0026quot;This task creates static application produced with cross compiler installed dockcross container (https://github.com/dockcross/dockcross).\u0026quot; } ] } Niestety, z dużym prawdopodobieństwem kompilator zawarty w kontenerze będzie niekompatybilny z kompilatorem użytym do wytworzenia rootfs. W efekcie łączenie w trybie dynamicznym nie będzie możliwe.\n3. Debugowanie skrośne Natywne debugowanie kodu z wykorzystaniem GUI pracującego na systemie target jest często awykonalne, głównie ze względu na ograniczony interfejs użytkownika i niezbyt duże zasoby urządzenia. Z kolei zdalna praca w konsoli tekstowej z debuggerem gdb jest doznaniem ekstremalnym.\nRozwiązaniem problemu jest podzielenie zadania uruchamiania na dwa oddzielne procesy. Proces gdbserver pracuje na systemie target i odpowiada za krokowe wykonywanie uruchamianej aplikacji. Z kolei interfejs użytkownika uruchomiony na systemie host komunikuje się z serwerem poprzez sieć lub łącze szeregowe (w dalszej części pojawią się konfiguracje dla łącza sieciowego). Programem uruchomionym na systemie host jest gdb-multiarch (jest to wersja gdb, która \u0026ldquo;rozumie\u0026rdquo; różne architektury). Oba komponenty komunikują się za pomocą protokołu TCP/IP. W efekcie struktura komunikacji pomiędzy narzędziami jest następująca: interfejs GUI uruchomiony w systemie host komunikuje się lokalnie z gdb-multiarch, który z kolei poprzez sieć komunikuje się z gdbserver uruchomionym na systemie target. GUI odpowiedzialne za przeprowadzenie sesji uruchamiania musi poinformować gdb-multiarch o architekturze uruchamianego kodu i adresie serwera oczekującego na połączenie. W Visual Studio Code za konfigurację procesu uruchamiania odpowiedzialny jest plik \u0026lsquo;.vscode/launch.json\u0026rsquo;. Dla rozważanego projektu przyjmie on postać\n{ \u0026quot;version\u0026quot;: \u0026quot;0.2.0\u0026quot;, \u0026quot;configurations\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;GDB-REMOTE\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;cppdbg\u0026quot;, \u0026quot;request\u0026quot;: \u0026quot;launch\u0026quot;, \u0026quot;program\u0026quot;: \u0026quot;${workspaceFolder}/main\u0026quot;, \u0026quot;miDebuggerServerAddress\u0026quot;: \u0026quot;opi.usb:1234\u0026quot;, \u0026quot;miDebuggerPath\u0026quot;: \u0026quot;/usr/bin/gdb-multiarch\u0026quot;, \u0026quot;targetArchitecture\u0026quot;: \u0026quot;arm64\u0026quot;, \u0026quot;args\u0026quot;: [], \u0026quot;stopAtEntry\u0026quot;: true, \u0026quot;cwd\u0026quot;: \u0026quot;${workspaceFolder}\u0026quot;, \u0026quot;environment\u0026quot;: [], \u0026quot;externalConsole\u0026quot;: false, \u0026quot;MIMode\u0026quot;: \u0026quot;gdb\u0026quot;, \u0026quot;setupCommands\u0026quot;: [ { \u0026quot;description\u0026quot;: \u0026quot;sets target architecture\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;-ex 'set architecture aarch64'\u0026quot;, \u0026quot;ignoreFailures\u0026quot;: true }, { \u0026quot;description\u0026quot;: \u0026quot;Enable pretty-printing for gdb\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;-enable-pretty-printing\u0026quot;, \u0026quot;ignoreFailures\u0026quot;: true } ] } ] } Sesję uruchamiania aktywuje się skrótem \u0026ldquo;F5\u0026rdquo; lub wybierając ikonę \u0026ldquo;żuczka\u0026rdquo; na pasku bocznym. Powyższe zadanie realizuje tylko komunikację lokalną z gdb-multiarch, tak więc każdorazowo przed rozpoczęciem sesji uruchamiania należy w terminalu Visual Studio Code wydać komendę\nscp ./main root@opi.usb: \u0026amp;\u0026amp; ssh -t root@opi.usb \u0026quot;gdbserver --once localhost:1234 ./main\u0026quot; Zadanie można sobie znacznie ułatwić, dopisując odpowiednie cele do \u0026lsquo;Makefile\u0026rsquo;\ndebug: $(APPNAME) scp $(APPNAME) $(REMOTE): \u0026amp;\u0026amp; ssh -t $(REMOTE) \u0026quot;gdbserver --once localhost:1234 ./$(APPNAME)\u0026quot; run: $(APPNAME) scp $(APPNAME) $(REMOTE): \u0026amp;\u0026amp; ssh -t $(REMOTE) \u0026quot;./$(APPNAME)\u0026quot; Podsumowanie Kompilacja skrośna własnego kodu nastręcza wielu problemów. Jeszcze więcej problemów pojawia się podczas kompilacji cudzych aplikacji. Profesjonalnie napisane aplikacje są wyposażone w mechanizm budowy dostosowujący je do środowiska kompilacji. Zazwyczaj budowanie aplikacji poprzedzonej jest uruchomieniem skryptu ./configurewygenerowanym przez środowisko automake. Zadaniem skryptu jest wykrycie dostępności pewnych składników systemie, ustawienie odpowiednich flag kompilacji i przygotowanie pliku Makefile dla programu make. Schemat ten działa poprawnie gdy system na którym ma być wykonywania aplikacja jest identycznie skonfigurowany (w sensie dostępności wykrywanych składników) z systemem na którym wykonywana jest aplikacja. W przypadku kompilacji skrośnej założenie to nie jest spełnione. Skrypt ./configure po prostu odpytuje niewłaściwy system. W efekcie ustawione opcje kompilacji są niepoprawne, co skutkuje błędami kompilacji, lub co gorsza, wygenerowaniem niewłaściwie działającego kodu. Ręczny dobór odpowiednich przełączników i adaptacja obcego kodu źródłowego jest zazwyczaj doznaniem ekstremalnym.\nW zasadzie wszystkie opisane problemy można rozwiązać stosując kompilację w trybie emulowanym.\nWady i zalety Budowanie natywne\nzalety wady możliwość tworzenie kodu łączonego dynamicznie konieczność instalacji narzędzi deweloperskich na urządzeniu target tworzenie kodu wymaga bezpośredniego dostepu do urządzenia docelowego Budowanie skrośne\nzalety wady kompilacja na urządzeniu host kod łączony dynamicznie może być przygotowany tylko w specyficznych sytuacjach możliwość hermetyzacji środowiska w kontenerze Przydatne pliki konfiguracyjne Makefile na wszystkie okazje\n# Compiler settings - Can be customized. PREFIX= STATIC= CC = $(PREFIX)gcc CXX =$(PREFIX)g++ CPPFLAGS = CFLAGS = -std=c11 -g -Wall -fdiagnostics-color=always CXXFLAGS = -std=c++11 -g -Wall -fdiagnostics-color=always # LDFLAGS = $(STATIC) -lpthread -lgpiod LDFLAGS = $(STATIC) ifdef SYSROOT CFLAGS += --sysroot=$(SYSROOT) CXXFLAGS += --sysroot=$(SYSROOT) LDLAGS += --sysroot=$(SYSROOT) endif # Makefile settings - Can be customized. APPNAME = main EXT = .c SRCDIR = src OBJDIR = obj REMOTE = root@opi.usb REMOTEPWD = /root/RemotePowerButton SRC = $(wildcard $(SRCDIR)/*$(EXT)) OBJ = $(SRC:$(SRCDIR)/%$(EXT)=$(OBJDIR)/%.o) DEP = $(OBJ:$(OBJDIR)/%.o=%.d) RM = rm -f all: $(APPNAME) # Copy sources to target .PHONY: copysrc copysrc: scp -p -r ./src/ ./obj/ ./Makefile $(REMOTE):$(REMOTEPWD) .PHONY: debug debug: $(APPNAME) scp $(APPNAME) $(REMOTE): \u0026amp;\u0026amp; ssh -t $(REMOTE) \u0026quot;gdbserver --once localhost:1234 ./$(APPNAME)\u0026quot; .PHONY: run run: $(APPNAME) scp $(APPNAME) $(REMOTE): \u0026amp;\u0026amp; ssh -t $(REMOTE) \u0026quot;./$(APPNAME)\u0026quot; # Builds the app $(APPNAME): $(OBJ) $(CC) $(CFLAGS) -o $@ $^ $(LDFLAGS) # Creates the dependecy rules %.d: $(SRCDIR)/%$(EXT) @$(CPP) $(CFLAGS) $(CPPFLAGS) $\u0026lt; -MM -MT $(@:%.d=$(OBJDIR)/%.o) \u0026gt;$@ # Includes all .h files -include $(DEP) # Building rule for .o files and its .c/.cpp in combination with all .h $(OBJDIR)/%.o: $(SRCDIR)/%$(EXT) $(CC) $(CFLAGS) $(CPPFLAGS) -o $@ -c $\u0026lt; # Cleans complete project .PHONY: clean clean: $(RM) $(OBJ) $(DEP) $(APPNAME) # Cleans only all files with the extension .d .PHONY: cleandep cleandep: $(RM) $(DEP) Wszystkie definicje zadań `.vscode/tasks.json\u0026rsquo;\n{ \u0026quot;version\u0026quot;: \u0026quot;2.0.0\u0026quot;, \u0026quot;tasks\u0026quot;: [ { \u0026quot;label\u0026quot;: \u0026quot;CrossBuild with static linkage\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;shell\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;make PREFIX=aarch64-linux-gnu- STATIC=-static\u0026quot;, \u0026quot;group\u0026quot;: { //\u0026quot;isDefault\u0026quot;: true, \u0026quot;kind\u0026quot;: \u0026quot;build\u0026quot; }, \u0026quot;options\u0026quot;: { \u0026quot;cwd\u0026quot;: \u0026quot;${workspaceFolder}\u0026quot; }, \u0026quot;problemMatcher\u0026quot;: [ \u0026quot;$gcc\u0026quot; ], \u0026quot;detail\u0026quot;: \u0026quot;This task creates static application produced with cross compiler installed on host.\u0026quot; }, { \u0026quot;label\u0026quot;: \u0026quot;CrossBuild with dynamic linkage\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;shell\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;make PREFIX=aarch64-linux-gnu- SYSROOT=/mnt/sysroot\u0026quot;, \u0026quot;group\u0026quot;: { //\u0026quot;isDefault\u0026quot;: true, \u0026quot;kind\u0026quot;: \u0026quot;build\u0026quot; }, \u0026quot;options\u0026quot;: { \u0026quot;cwd\u0026quot;: \u0026quot;${workspaceFolder}\u0026quot; }, \u0026quot;problemMatcher\u0026quot;: [ \u0026quot;$gcc\u0026quot; ], \u0026quot;detail\u0026quot;: \u0026quot;This task creates dynamically linked application produced with cross compiler installed on host. Targets rootfs is visible in /mnt/sysroot.\u0026quot; }, { \u0026quot;label\u0026quot;: \u0026quot;Build Natively on Target\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;shell\u0026quot;, \u0026quot;command\u0026quot;: \u0026quot;make copysrc ; ssh root@opi.usb '(cd /root/RemotePowerButton; make)'\u0026quot;, \u0026quot;group\u0026quot;: { //\u0026quot;isDefault\u0026quot;: true, \u0026quot;kind\u0026quot;: \u0026quot;build\u0026quot; }, \u0026quot;options\u0026quot;: { \u0026quot;cwd\u0026quot;: \u0026quot;${workspaceFolder}\u0026quot; }, \u0026quot;problemMatcher\u0026quot;: [ \u0026quot;$gcc\u0026quot; ], \u0026quot;detail\u0026quot;: \u0026quot;This task copies project sources to target and then it builds them natively there.\u0026quot; } ] } Konfiguracja IntelliSense w \u0026lsquo;.vscode/c_cpp_properties.json\u0026rsquo;\n{ \u0026quot;configurations\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;NativeOverSSH\u0026quot;, \u0026quot;includePath\u0026quot;: [ \u0026quot;${workspaceFolder}/src/**\u0026quot;, \u0026quot;${workspaceFolder}/rootfs/**\u0026quot; ], \u0026quot;defines\u0026quot;: [ // symbols resolved by the preprocessor that are unknown to IntelliSense \u0026quot;NULL=0\u0026quot; // I suspect that \u0026quot;# define NULL 0\u0026quot; in types.h is accepted by cpp, but IGNORED by IntelliSense ], \u0026quot;compilerPath\u0026quot;: \u0026quot;\u0026quot;, \u0026quot;cStandard\u0026quot;: \u0026quot;c11\u0026quot;, \u0026quot;cppStandard\u0026quot;: \u0026quot;c++11\u0026quot;, \u0026quot;intelliSenseMode\u0026quot;: \u0026quot;linux-gcc-arm64\u0026quot; } ], \u0026quot;version\u0026quot;: 4 } Konfiguracja sesji uruchamiania w \u0026lsquo;.vscode/launch.json\u0026rsquo;\n{ \u0026quot;version\u0026quot;: \u0026quot;0.2.0\u0026quot;, \u0026quot;configurations\u0026quot;: [ { \u0026quot;name\u0026quot;: \u0026quot;GDB-REMOTE\u0026quot;, \u0026quot;type\u0026quot;: \u0026quot;cppdbg\u0026quot;, \u0026quot;request\u0026quot;: \u0026quot;launch\u0026quot;, \u0026quot;program\u0026quot;: \u0026quot;${workspaceFolder}/main\u0026quot;, \u0026quot;miDebuggerServerAddress\u0026quot;: \u0026quot;opi.usb:1234\u0026quot;, \u0026quot;miDebuggerPath\u0026quot;: \u0026quot;/usr/bin/gdb-multiarch\u0026quot;, \u0026quot;targetArchitecture\u0026quot;: \u0026quot;arm64\u0026quot;, \u0026quot;args\u0026quot;: [], \u0026quot;stopAtEntry\u0026quot;: true, \u0026quot;cwd\u0026quot;: \u0026quot;${workspaceFolder}\u0026quot;, \u0026quot;environment\u0026quot;: [], \u0026quot;externalConsole\u0026quot;: false, \u0026quot;MIMode\u0026quot;: \u0026quot;gdb\u0026quot;, \u0026quot;setupCommands\u0026quot;: [ { \u0026quot;description\u0026quot;: \u0026quot;sets target architecture\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;-ex 'set architecture aarch64'\u0026quot;, \u0026quot;ignoreFailures\u0026quot;: true }, { \u0026quot;description\u0026quot;: \u0026quot;Enable pretty-printing for gdb\u0026quot;, \u0026quot;text\u0026quot;: \u0026quot;-enable-pretty-printing\u0026quot;, \u0026quot;ignoreFailures\u0026quot;: true } ] } ] } 4. Kruczki i sztuczki 4.1 Zdalny dostęp do konta administratora bez podawania hasła Zdalny dostęp do konta administratora\nW pliku /etc/ssh/sshd_config\u0026rsquo; (target) należy ustawić opcję\nPermitRootLogin yes Logowanie bez konieczności podawania hasła\nSerwer ssh musi zezwalać na uwierzytelnienie za pomocą kluczy publicznych. W pliku /etc/ssh/sshd_config\u0026rsquo; (target) należy ustawić opcje\nPubkeyAuthentication yes AuthorizedKeysFile .ssh/authorized_keys .ssh/authorized_keys2 Użytkownik systemu host musi mieć wygenerowane osobiste klucze ssh. Generacja komendą\nssh-keygen Opcją \u0026lsquo;-t\u0026rsquo; można wymusić typ klucza, a opcją \u0026lsquo;-b\u0026rsquo; jego długość. Wygenerowane klucze, prywatny i publiczny, umieszczone są w podkatalogu \u0026lsquo;.ssh\u0026rsquo; katalogu domowego (publiczny ma rozszerzenie \u0026lsquo;pub\u0026rsquo;).\nKlucz publiczny należy umieścić na systemie target w pliku \u0026lsquo;.ssh/authorized_keys\u0026rsquo; na koncie docelowym. Wygodne kopiowanie zapewnia komenda (poniżej użyto konta root)\nssh-copy-id root@opi.usb 4.2 Udostępnianie głównego systemu plików poprzez NFS System target Instalacja serwera NFS.\napt install -y nfs-kernel-server Udostępnienie katalogu \u0026lsquo;/usr\u0026rsquo; w trybie tylko do odczytu\nZatrzymanie serwera\nsystemctl stop nfs-kernel-server Aktualizacja \u0026lsquo;/etc/exports\u0026rsquo;\n/usr *(ro,sync,no_subtree_check) Uruchomienie serwera\nexportsfs -ra systemctl start nfs-kernel-server System host Utworzenie punktu montowania\nmkdir -p /mnt/sysroot/usr Montowanie zdalnego folderu na żądanie. W \u0026lsquo;/etc/fstab\u0026rsquo;\nopi.usb:/usr /mnt/sysroot/usr nfs noauto,nofail,noatime,nolock 0 0 Montowanie komendą\nsudo mount /mnt/sysroot/usr Kompilator należy poinformować, że katalogiem \u0026lsquo;sysroot\u0026rsquo; jest \u0026lsquo;/mnt/sysroot\u0026rsquo;.\nP.S. Chętnie się dowiem jak prosto udostępnić \u0026ldquo;/\u0026rdquo; w którym są linki symboliczne np. bin-\u0026gt;/usr/bin.\n","date":"2023-08-12","permalink":"https://pejotzet.github.io/pejotzet/post/2023-08-12-c-cpp-development-for-sbc-in-code/","tags":["C/C++","SBC","Docker","DevContainers"],"title":"Jak kompilować i debugować aplikacje w C/C++ na komputery jednopłytkowe korzystając z VS Code"},{"content":" ESP-Prog jest programatorem i debuggerem do platformy ESP32. Niniejszy dokument opisuje jak korzystać z tego urządzenia w systemie Linux z zainstalowanym edytorem Code wyposażonym w rozszerzenie PlatformIO.\nAnatomia ESP-Prog Płyta ESP-Prog jest wyposażona w dwa złącza służące do komunikacji z urządzeniem docelowym.\nZłącze PROG służy do wgrania nowej wersji firmware\u0026rsquo;u na urządzenie docelowe, natomiast złącze JTAG umożliwia krokowe wykonanie programu, inspekcję pamięci urządzenia itp. Do pinów złącz przypisane są następujące sygnały\nPodłączenie obu interfejsów do urządzenia docelowego powoduje, że ESP-Prog przejmuje nad nim pełną kontrolę. W takim podejściu jedynym urządzeniem komunikującym się z komputerem PC jest ESP-Prog. W drugiej wersji konfiguracji z urządzeniem docelowym łączymy tylko sygnały ze złącza JTAG. Do aktualizacji firmware\u0026rsquo;u wykorzystywany jest wtedy natywny interfejs urządzenia docelowego, natomiast złącze JTAG służy tylko i wyłącznie do kontroli wykonywanego kodu.\nNa koniec kilka uwag nt. zasilania urządzenia docelowego. Obok złącz znajdują się trójpinowe zworki. Ich odpowiednie zwarcie ustawia żądany poziom napięcia na pinie VDD każdego ze złącz. Napięcie 3.3V jest brane z wyjścia regulatora na ESP-Prog i może być połączone z pinem 3.3V na płycie docelowej. Natomiast napięcie 5V pochodzi bezpośrednio ze złącza USB (sprzed regulatora) i musi być łączone z pinem 5V na płycie docelowej. Zadanie stabilizacji napięcia 3.3V na płycie docelowej przejmie wtedy jej własny regulator. Oczywiście, nie ma wtedy potrzeby łączenia 3.3V z ESP-Prog z płytą docelową. Warto zauważyć, że łączenie napięć można zupełnie pominąć i zasilić urządzenie docelowe z niezależnego źródła np. ładowarki telefonu. Oczywiście masy urządzeń powinny być obowiązkowo połączone. Szczegółowy opis właściwości układu jest dostępny na jego stronie domowej [1].\nPrzygotowanie systemu Linux Przygotowanie systemu przebiega dokładnie tak samo, jak w przypadku pracy z dowolnym mikrokontrolerem opartym o układ ESP32. Po podłączeniu takiego mikrokontrolera w systemie pojawiają się urządzenia /dev/ttyUSBx, gdzie x należy zastąpić numerem kolejnego urządzenia. Konfiguracja systemu sprowadza się do nadania praw dostępu do tych urządzeń nieuprzywilejowanym użytkownikom.\nDostęp dla wszystkich użytkowników Instalacja odpowiednich reguł dla demona udev zapewnia dostęp world-wide do plików tworzonych przez sterowniki urządzeń USB o identyfikatorach przypisanych emulatorom łącza szeregowego.\ncurl -fsSL https://raw.githubusercontent.com/platformio/platformio-core/develop/platformio/assets/system/99-platformio-udev.rules | sudo tee /etc/udev/rules.d/99-platformio-udev.rules sudo udevadm control --reload-rules sudo udevadm trigger Dostęp tylko dla wybranych użytkowników Bardziej subtelna kontrola dostępu wykorzystuje mechanizm przynależności użytkowników systemu do odpowiednich grup. Domyślnie pliki /dev/ttyUSBx mają prawa dostępu postaci 0660 i należą do root:dialup (Ubuntu) lub root:uucp (Arch) . Zatem wyróżnionych użytkowników należy dodać do grupy umożliwiającej dostęp do plików urządzeń oraz grupy kontrolującej prawa dostępu do urządzeń dynamicznie łączonych z systemem\nUbuntu\u0026gt; sudo usermod -aG plugdev $USER ; sudo usermod -aG dialup $USER Arch\u0026gt; sudo usermod -aG lock $USER ; sudo usermod -aG uucp $USER Dodatkowe oprogramowanie W dalszych rozważaniach będzie omawiana konfiguracja VS Code z rozszerzeniem PlatformIO, zatem instalacja tego oprogramowania jest niezbędna. Podczas uruchamiania (debugowania) programów wykonywany jest kod .platformio/packages/toolchain-xtensa-esp32/bin/xtensa-esp32-elf-gdb, który wymaga biblioteki libpython2.7.so, co w normalnym języku oznacza, że konieczna jest instalacja Pythona w wersji 2. W Ubuntu nie jest to problemem, natomiast Arch Linux w oficjalnych repozytoriach nie zawiera już tego oprogramowania. Jego instalacja wymaga otwarcia kanału z oprogramowaniem zarządzanym przez ochotników (tzw. kanał aur) i zbudowanie pakietu ze źródeł. Działania te można bezproblemowo wykonać z poziomu standardowego zarządcy pakietów.\nKonfiguracja Master-Slave Rolę Master pełni ESP-Prog a urządzenie Slave zrealizowano jako ESP32 DevKitCv4.\nCzęść sprzętowa Połączenie złącza `PROG`` z ESP32\ndevice connector pin number pin name pin name device ESP-PROG PROGRAM 1 ESP_EN EN ESP32 ESP-PROG PROGRAM 2 VDD $3.3V/5V^*$ ESP32 ESP-PROG PROGRAM 3 ESP_TXD TX ESP32 ESP-PROG PROGRAM 4 GND GND ESP32 ESP-PROG PROGRAM 5 ESP_RXD RX ESP32 ESP-PROG PROGRAM 6 ESP_IO0 $GPIO 0^{**}$ ESP32 $^*$ \u0026ndash; zgodnie z napięciem wybranym na zworce przy złączu\n$^{**}$ \u0026ndash; tylko gdy zwarta jest zworka przy złączu\nPołączenie złącza JTAG z ESP32\ndevice connector pin number pin name pin name device ESP-PROG JTAG 1 VDD $3.3V/5V^1$ ESP32 ESP-PROG JTAG 2 ESP_TMS GPIO 14 ESP32 ESP-PROG JTAG 3 GND GND ESP32 ESP-PROG JTAG 4 ESP_TCK GPIO 13 ESP32 ESP-PROG JTAG 6 ESP_TDO GPIO 15 ESP32 ESP-PROG JTAG 8 ESP_TDI GPIO 12 ESP32 $^1$ \u0026ndash; zgodnie z napięciem wybranym na zworce przy złączu\nUwaga!!! Piny urządzenia docelowego wykorzystywane do komunikacji z ESP-Prog nie powinny być kontrolowane przez firmware urządzenia docelowego.\nOgraniczenie to dotyczy nie tylko nowej wersji, ale również zastępowanego firmware\u0026rsquo;u. Gdy na urządzeniu docelowym jest zainstalowane nieznane oprogramowanie, to należy je zastąpić wersją spełniającą niniejszy wymóg korzystając ze standardowej metody aktualizacji.\nCzęść programowa Adaptacja pliku projektu Niech pierwotna konfiguracja projektu w platform.ini ma postać\n[env:esp32doit-devkit-v1] board = esp32doit-devkit-v1 monitor_speed = 115200 platform = espressif32 framework = arduino lib_deps = ayushsharma82/AsyncElegantOTA@^2.2.7 esphome/AsyncTCP-esphome@^2.0.0 esphome/ESPAsyncWebServer-esphome@^3.0.0 Wystarczy do niej dopisać następujący fragment\n... [env:esp32doit-devkit-v1-debug] extends = env:esp32doit-devkit-v1 debug_tool = esp-prog debug_init_break = tbreak setup [platformio] default_envs = esp32doit-devkit-v1-debug Sekcja [platformio] zapewnia, że nowa konfiguracja stanie się domyślną. W nowej konfiguracji pierwotną wersję uzupełniono tylko dwoma wpisami: debug-tool określa rodzaj urządzenia wykorzystywanego do uruchamiania, natomiast postać debug_init_break ustawia punkt zatrzymania programu (breakpoint) na początku procedury setup.\nWgrywanie nowego firmware Po prostu wystarczy wybrać opcję Upload, tak jak to zwykle się czyni. Zielona dioda na ESP-Prog powinna zacząć mrugać sygnalizując proces wgrywania nowej wersji oprogramowania.\nProcedura uruchamiania Na panelu bocznym klikamy ikonę żuczka (skrót Ctrl-Alt-U) i ze spuszczanej listy wybieramy konfigurację PIO Debug, gdy chcemy zaktualizować firmware przed rozpoczęciem uruchamiania, lub PIO Debug (without uploading), gdy chcemy przejść bezpośrednio do procedury uruchamiania kodu zainstalowanego na urządzeniu docelowym. Uruchamianie można zainicjować naciskając przycisk Play obok nazwy konfiguracji lub wykorzystując skrót klawiszowy F5. Komunikacja z urządzeniem docelowym jest dość wolna. Po chwili na ekranie powinno pojawić się menu debugera , które obsługujemy normalnie. W oknie kodu źródłowego można ustawiać punkty przerwań wykonania programu, klikając po lewej stronie numeru linii.\nKonfiguracja Remote Debug Część sprzętowa W tej konfiguracji zarówno urządzenie docelowe (DevKitCv4) jak i ESP-Prog połączone są z komputerem PC niezależnie. W konfiguracji wykorzystywane jest tylko złącze JTAG do uruchamiania kodu działającego na urządzeniu docelowym (DevKitCv4). Złącze PROG pozostaje niewykorzystane. Nie ma potrzeby łączenia pinu VDD ponieważ każde urządzenie jest zasilane niezależnie.\nPodłączenie każdego z urządzeń do komputera PC powoduje pojawienie się nowych plików urządzeń: DevKitC \u0026ndash; jednego, ESP-Prog \u0026ndash; dwóch. Ich numeracja zależy od kolejności podłączania. Przyjmijmy, że DevKitC został podłączony jako pierwszy. Firmware można do niego zatem wgrać za pomocą interfejsu /dev/ttyUSB0. Podłączenie ESP-Prog powoduje pojawienie się w systemie dwóch kolejnych urządzeń: /dev/ttyUSB1 i /dev/ttyUSB2.\nAdaptacja pliku projektu Niech pierwotna konfiguracja projektu w platform.ini ma postać\n[env:esp32doit-devkit-v1] board = esp32doit-devkit-v1 monitor_speed = 115200 platform = espressif32 framework = arduino lib_deps = ayushsharma82/AsyncElegantOTA@^2.2.7 esphome/AsyncTCP-esphome@^2.0.0 esphome/ESPAsyncWebServer-esphome@^3.0.0 Wystarczy do niej dopisać następujący fragment\n... [env:esp32doit-devkit-v1-debug-standalone] extends = env:esp32doit-devkit-v1 upload_port = /dev/ttyUSB0 upload_protocol = esptool monitor_port = /dev/ttyUSB0 debug_tool = esp-prog debug_init_break = tbreak setup [platformio] default_envs = esp32doit-devkit-v1-debug-standalone Jak widać, zmiana w stosunku do poprzedniej metody uruchamiania polega jedynie na jawnej specyfikacji urządzenia wykorzystywanego do aktualizacji firmware\u0026rsquo;u i obserwacji konsoli szeregowej urządzenia.\nProcedura uruchamiania Na panelu bocznym klikamy ikonę żuczka (skrót Ctrl-Alt-U) i ze spuszczanej listy wybieramy (koniecznie) PIO Debug (without uploading). Następnie uruchamiany sesję debuggera wybierając przycisk Play lub skrót klawiszowy F5. Reszta procedury przebiega identycznie jak w poprzednim przypadku.\nPodsumowanie ESP-Prog jest dedykowanym programatorem i debuggerem dedykowanym dla platformy ESP32. Oferuje on stabilne i wydajne rozwiązanie do programowania i debugowania układów ESP32. Jego najważniejsze zalety to:\nWsparcie dla JTAG i UART. ESP-Prog umożliwia zarówno debugowanie za pomocą interfejsu JTAG jak i programowanie z wykorzystaniem interfejsu UART.\nDebugowanie w czasie rzeczywistym: Dzięki ESP-Prog można wykonywać debugowanie w czasie rzeczywistym, co ułatwia identyfikację i rozwiązywanie problemów w kodzie podczas jego wykonywania na platformie ESP32.\nŁatwa integracja z PlatformIO: ESP-Prog jest dobrze zintegrowany z PlatformIO - popularnym narzędziem do rozwoju oprogramowania dla układów mikrokontrolerów. Umożliwia to wygodne i efektywne programowanie i debugowanie projektów ESP32 w środowisku PlatformIO, w szczególności w Visual Studio Code.\nWsparcie dla różnych płytek deweloperskich: ESP-Prog jest kompatybilny z różnymi płytkami deweloperskimi ESP32, co oznacza, że ​​można go używać z wieloma różnymi układami ESP32 bez konieczności zakupu wielu różnych programatorów.\nBibliografia [1] https://docs.espressif.com/projects/espressif-esp-iot-solution/en/latest/hw-reference/ESP-Prog_guide.html\n","date":"2023-08-12","permalink":"https://pejotzet.github.io/pejotzet/post/2023-08-12-how-to-debug-esp32-with-esp-prog/","tags":["Code","PlatformIO","ESP32","ESP-Prog"],"title":"Jak uruchamiać programy dla ESP32 korzystając z ESP-Prog"},{"content":"W poradniku pokazano jak wykorzystać konteneryzację i wsparcie dla niej w Visual Studio Code do realizacji statycznego serwisu WWW. Opis obejmuje wykorzystanie kontenerów do stworzenia hermetycznego i przenośnego środowiska pracy oraz hosting lokalny i z użyciem GitHub. Przedstawiona metoda lokalnego hostingu umożliwia dostęp do serwisu za pomocą bezpiecznego protokołu HTTPS i umożliwia korzystanie z usług dynamicznego rozwiązywania nazw gdy adres IP zrealizowanego serwera jest zmienny.\nCzynności wstępne Praktyczna realizacja porad wymaga zainstalowania Git, Docker\u0026lsquo;a w wersji desktop lub serwer i Code oraz posiadania kont na GitHub i opcjonalnie DuckDNS.\nPrzygotowanie folderu projektu Utwórz puste repozytorium GitHub. Repozytorium będzie wykorzystywane do przechowywania plików generatora Hugo oraz plików źródłowych. Jeżeli konto GitHub ma być wykorzystywane również do publikacji gotowego serwisu oraz automatycznej generacji stron po aktualizacji zawartości serwisu, to powinno być ustawione jako publiczne. W kontach prywatnych funkcjonalności te są dostępne tylko za opłatą.\nSklonuj repozytorium do lokalnego folderu. Procedura klonowania zapewnia, że katalog projektu jest już w zasadzie zainicjowany jako repozytorium Git, tzn. istnieje podkatalog .git/ zawierający niezbędne metadane. W trakcie pracy nad serwisem zawartość katalogu projektu będzie podlegała zmianom. Utrzymanie synchronizmu z repozytorium GitHub wymaga nadążnej aktualizacji lokalnego folderu ze zmianami jakie zaszły w zdalnym repozytorium (git fetch) oraz okresowego wypychania zmian naniesionych lokalnie do zdalnego repozytorium (git stage, git commit, git push). Działania te będą wykonywane półautomatycznie przez VS Code. Jednak poprawne działanie klienta Git wymaga przypisania do lokalnego repozytorium pseudonimu i adresu email użytkownika, który nanosi w nim zmiany. Dlatego w katalogu projektu po jego utworzeniu należy wydać komendy\ngit config user.name \u0026quot;your_name\u0026quot; git config user.email \u0026quot;your_mail@somewhere.com\u0026quot; Zainstaluj kontener deweloperski zawierający gotowe środowisko pracy. Edytor Code umożliwia korzystanie z kontenerów Docker\u0026rsquo;a zawierających gotowe i hermetyczne środowiska pracy. Mechanizm ten nazwano DevContainers. Code po uruchomieniu w katalogu odpowiednio skonfigurowanego projektu \u0026ldquo;wchodzi\u0026rdquo; do wnętrza kontenera, a katalog projektu jest mapowany na katalog roboczy wewnątrz kontenera. Dla dewelopera automatycznie wszystkie narzędzia zainstalowane w kontenerze stają się dostępne. Jednocześnie wprowadzone przez niego zmiany ograniczone są do plików projektu oraz plików kontenera, przy czym te drugie są ulotne i znikają po skasowaniu kontenera. Tylko zmiany naniesione w katalogu roboczym mają charakter trwały. Dla wielu typów projektów szablony środowisk pracy są już przygotowane i publicznie udostępnione. W omawianym przypadku wykorzystamy środowisko do pracy ze statycznym generatorem stron o nazwie Hugo. Jest to jeden z najlepszych generatorów tego typu. W tym celu należy z podkatalogu containers/hugo repozytorium https://github.com/microsoft/vscode-dev-containers/ pobrać wraz zawartością katalog .devcontainer/ i opcjonalnie .vscode/. Zadanie to realizuje komenda\ncurl -Ls https://github.com/microsoft/vscode-dev-containers/archive/main.tar.gz | tar -xvz --strip-components=3 --wildcards '*/*/hugo/.*' Ze względu na niewielką liczbe importowanych plików zadanie to można wykonać \u0026ldquo;ręcznie\u0026rdquo; z użyciem przeglądarki WWW. Opisane wyżej zadania realizuje poniższy skrypt\n#!/bin/sh gh_user=your_gh_account gh_repo=your_gh_repo gh_name=your_name gh_mail=your_mail@somewhere.com git clone https://github.com/${gh_user}/${gh_repo} cd ${gh_repo} git config user.name \u0026quot;${gh_name}\u0026quot; git config user.email \u0026quot;${gh_mail}\u0026quot; curl -Ls https://github.com/microsoft/vscode-dev-containers/archive/main.tar.gz | tar -xvz --strip-components=3 --wildcards '*/*/hugo/.*' Przed pierwszym uruchomieniem warto zapoznać się z treścią plików .devcontainer/devcontainer.json i .devcontainer/Dockerfile, które definiują konfigurację kontenera i przepis na jego zbudowanie. Zmiany w pliku Dockerfile zazwyczaj nie są konieczne. Natomiast w devcontainer.json warto ustawić wersję Hugo na extended, wersję node na najwyższą wspieraną w Dockerfile, ustawić nazwę kontenera korzystając z runArgs, poprosić o wypisanie w terminalu wersji zainstalowanych narzędzi po utworzeniu kontenera, zmienić rozszerzenie wykorzystywane do obsługi plików TOML z bungcip.better-toml na tamasfe.even-better-toml, dołożyć sprawdzanie pisowni i opcjonalnie dodać rozszerzenie do parsowania plików definiujących GitHub Actions (więcej o tym później). Po wspomnianych modyfikacjach plik devcontainer.json przyjmie postać\n{ \u0026quot;name\u0026quot;: \u0026quot;Hugo (Community)\u0026quot;, \u0026quot;build\u0026quot;: { \u0026quot;dockerfile\u0026quot;: \u0026quot;Dockerfile\u0026quot;, \u0026quot;args\u0026quot;: { \u0026quot;VARIANT\u0026quot;: \u0026quot;hugo_extended\u0026quot;, \u0026quot;VERSION\u0026quot;: \u0026quot;latest\u0026quot;, \u0026quot;NODE_VERSION\u0026quot;: \u0026quot;18\u0026quot; } }, \u0026quot;runArgs\u0026quot;: [\u0026quot;--name=this-is-Hugo-DevContainer\u0026quot;] , \u0026quot;customizations\u0026quot;: { \u0026quot;vscode\u0026quot;: { \u0026quot;settings\u0026quot;: { \u0026quot;html.format.templating\u0026quot;: true, \u0026quot;cSpell.language\u0026quot;: \u0026quot;en, pl\u0026quot;, \u0026quot;cSpell.userWords\u0026quot;: [\u0026quot;tutaj\u0026quot;, \u0026quot;wpisz\u0026quot;, \u0026quot;wyjątki\u0026quot;, \u0026quot;dla\u0026quot;, \u0026quot;słów\u0026quot;, \u0026quot;polskich\u0026quot;] }, \u0026quot;extensions\u0026quot;: [ \u0026quot;tamasfe.even-better-toml\u0026quot;, \u0026quot;davidanson.vscode-markdownlint\u0026quot;, \u0026quot;streetsidesoftware.code-spell-checker-polish\u0026quot;, \u0026quot;GitHub.vscode-github-actions\u0026quot; ]\t} }, \u0026quot;forwardPorts\u0026quot;: [ 1313 ], \u0026quot;postCreateCommand\u0026quot;: \u0026quot;uname -a \u0026amp;\u0026amp; hugo version \u0026amp;\u0026amp; node --version\u0026quot;, \u0026quot;remoteUser\u0026quot;: \u0026quot;node\u0026quot; } Teraz można uruchomić Code wydając w katalogu projektu komendę\ncode . Code \u0026ldquo;zauważy\u0026rdquo;, że istnieje podkatalog .devcontainer/ i zapyta, czy otworzyć projekt w kontenerze deweloperskim. Należy się na to zgodzić. Przełączenie pomiędzy pracą lokalną a kontenerem (traktowanym przez Code jak zdalna maszyna) aktywuje się za pomocą przycisku znajdującego się w lewym dolnym rogu interfejsu Code .\nPrzygotowanie szablonu serwisu Po uruchomieniu Code i wejściu do wnętrza kontenera (pierwsze uruchomienie nie jest natychmiastowe, gdyż kontener musi zostać zbudowany z komponentów pobieranych z sieci) należy uruchomić terminal (\u0026lsquo;Ctrl-Shift-`\u0026rsquo;) i zainicjować pusty serwis\nhugo new site . --force Praca z tak przygotowanym środowiskiem wymaga znacznego nakładu pracy i znajomości zasad generacji stron oraz przygotowanie szablonów HTML, które będą wypełniane treścią podczas generacji serwisu. Na szczęście można skorzystać z szablonów serwisów przygotowanych przez ochotników i udostępnionych na https://themes.gohugo.io/themes/. W dalszej części wykorzystamy szablon Fuji. Jego instalacja sprowadza się do wydania w katalogu projektu komendy\ngit submodule add https://github.com/dsrkafuu/hugo-theme-fuji.git themes/fuji Wszystkie prawidłowo przygotowane style serwisów zawierają gotowy przykład. Wystarczy go przenieść do budowanego serwisu, aby sprawdzić działanie przygotowanego środowiska pracy\ncp -r themes/fuji/exampleSite/* . Plik konfiguracyjny obecnej wersji Hugo nosi nazwę hugo.toml. Wiele szablonów korzysta ze starszej konwencji i opiera się o plik config.toml. Na szczęście zachowana jest kompatybilność wstecz, więc wystarczy skasować hugo.toml.\nrm hugo.toml Na obecnym etapie można już wygenerować serwis wydaną w terminalu komendą\nhugo server --disableFastRender --buildDrafts Tak uruchomiony generator będzie śledził zmiany w plikach wejściowych i \u0026ldquo;na bieżąco\u0026rdquo; generował zawartość serwisu. Po wydaniu powyższej komendy Code zaproponuje podgląd serwisu w przeglądarce systemowej lub wbudowanej w Code.\nWięcej informacji na temat dostepnych przełączników mozna uzyskać komendą\nhugo server --help Przygotowanie treści serwisu Zadaniem generatora Hugo jest przekształcenie zbioru plików wejściowych w formacie Markdown w gotowy serwis WWW. Translacja Markdown na HTML odbywa się zgodne z szablonami zdefiniowanymi w stylu serwisu. Hugo wymaga, aby pliki źródłowe były umieszczone w katalogu content/. Są one pogrupowane w zależności od typu informacji zamieszczonej w serwisie. Rodzaje publikowanych informacji są zdefiniowane w ramach stylu, natomiast prawie zawsze występuje typ post/ reprezentujący wpis blogu. Każdy plik wejściowy jest opatrzony metadanymi zawierającymi tytuł, datę utworzenia itp. Sposób formatowania tych metadanych zawierają pliki zaimportowane z przykładowego serwisu. W kontekście dalszej pracy istotna jest flaga draft\n--- ... draft: true --- Pliki z tą flagą są wyłączane z zawartości serwisu przeznaczonego do publikacji. Zatem, aby uniknąć publikacji zaimportowanych przykładowych plików należy w każdym z nich ustawić tę flagę.\nKolejnym elementem, którego edycja jest niezbędna, jest plik config.toml. Należy w nim ustawić URL pod którym będzie dostępny serwis. Jeżeli wartość zmiennej baseURL jest ustawiona na\nbaseURL = \u0026quot;https://your_gh_account.github.io/gh_repo/\u0026quot; to podgląd generowanych na bieżąco stron będzie dostępny pod adresem http://localhost:1313/gh_repo. Plik konfiguracyjny zawiera również wiele innych zmiennych, których nazwy i wartości silnie zależą od użytego stylu serwisu.\nOstatnim z obowiązkowych kroków jest przygotowanie nowego wpisu. Przyjmiemy konwencję, że każdy post będzie zlokalizowany w osobnym folderze. W folderze tym będą również składowane zasoby skojarzone z postem, takie jak np. grafika. Nowy post zakłada się komendą\nhugo new post/to-moj-pierwszy-wpis/index.md Plik index.md zawiera już niektóre metadane. Szablon wg którego został przygotowany znajduje się w pliku archetypes/default.md. Domyślnie plik index.md ma postać\n--- title: \u0026quot;To Moj Pierwszy Wpis\u0026quot; date: 2023-08-05T17:18:23Z draft: true --- Plik ten należy uzupełnić treścią. Przy formatowaniu wpisu (treść, metadane) najlepiej kierować się przykładami zawartymi w szablonie serwisu. W niektórych szablonach streszczenia nowych wpisów pojawiają się na stronie głównej serwisu. Streszczenie oddziela się od treści głównej umieszczając w pliku wejściowym Markdown znacznik read more.\nWięcej informacji nt. prawidłowego przygotowania plików dla generatora jest dostępnych na stronie projektu Hugo. Przykłady zawarte w użytym szablonie serwisu również mogą być pomocne.\nPraca ze zdalnym repozytorium Zdalne repozytorium GitHub może wykorzystać trojako: do synchronizacji katalogów roboczych na kilku różnych komputerach, do stworzenia kopii zapasowej projektu oraz do automatycznego generowania serwisu i jego publikacji. Ostatniemu zagadnieniu poświęcono osobną sekcję.\nPraca z repozytorium GitHub w Code nie nastręcza żadnych trudności bowiem w edytor wbudowano odpowiednie mechanizmy. Pliki, które zmieniono względem ostatniej synchronizacji ze zdalnym repozytorium są oznaczane literką \u0026ldquo;U\u0026rdquo;. Aby zsynchronizować się zdalnym repozytorium należy na panelu bocznym wybrać ikonę współdzielenia. Synchronizacja odbywa się w paczkach zwanych \u0026ldquo;Commit\u0026rdquo;. Każdy \u0026ldquo;Commit\u0026rdquo; jest opatrzony krótkim opisem, zatem mechanizm ten umożliwia aktualizację repozytorium w sposób zorientowany problemowo. Aby plik znalazł się w danej paczce \u0026ldquo;Commit\u0026rdquo;, musi najpierw zostać przeniesiony do poczekalni (tzw. stage). Po wybraniu ikony współdzielenia prezentowana jest lista zmienionych plików projektu. Z listy tej można wybrać pliki, które zostają wystawione do aktualizacji (stage). Następnie, w górnej linii panelu bocznego należy wpisać opis paczki, a następnie wybrać \u0026ldquo;Commit\u0026rdquo;. Paczek \u0026ldquo;Commit\u0026rdquo; można utworzyć kilka. Aktualizację ze zdalnym repozytorium realizuje wybranie przycisku \u0026ldquo;Sync\u0026rdquo;.\nGit umożliwia wyłączenie wybranych plików z mechanizmu synchronizacji. Są one określone w pliku .gitignore, przy czym dozwolone jest stosowanie standardowych masek powłoki w postaci znaków \u0026ldquo;?\u0026rdquo; i \u0026ldquo;*\u0026rdquo;. Typowa postać tego pliku dla repozytorium zawierające projekt statycznego serwisu Hugo ma postać\nresources/_gen/* themes/*/exampleSite/* .vscode/ public/* Katalog resources/ zawiera pliki pośrednie wykorzystane przez generator, natomiast katalog public/ pliki wynikowe. Jeżeli repozytorium GitHub ma być wykorzystane do publikacji gotowego serwisu wygenerowanego lokalnie, to oczywiście katalog public/ należy usunąć z pliku .gitignore.\nPublikacja serwisu Serwis GitHub, podobnie do GitLab i kilku innych serwisów wspierających pracę grupową nad projektami, dostarcza usługi publikacji zasobów repozytorium oraz możliwość ciągłej integracji zmian i dostarczania nowych wersji oprogramowania (CI/CD). W serwisie GitHub usługi te noszą odpowiednio nazwy GitHub Pages i GitHub Actions. GitHub Pages polega na potraktowaniu jednego z katalogów repozytorium jako serwisu WWW i udostępnienia go pod adresem\nhttps://your_gh_account.github.io/gh_repo/ Z kolei za pomocą GitHub Actions można skonfigurować wyzwalanie pewnych działań na zawartości serwisu gdy zostaną spełnione określone warunki, np. gdy pliki repozytorium zostaną zaktualizowane. Pierwotnie usługi GitHub Pages i GitHub Actions funkcjonowały niezależnie. Jednym z dostępnych działań jest możliwość opublikowania określonego folderu repozytorium jako serwisu WWW. Obecnie GitHub Pages jest realizowana jako jedno z działań GitHub Actions, jednak, ze względów historycznych ma wydzieloną specjalną pozycję w interfejsie repozytorium. Poza konfiguracją mechanizmu CI/CD żadna dodatkowa konfiguracja nie jest potrzebna do publikacji serwisu. Należy jednak pamiętać, że usługi GitHub Actions i GitHub Pages są dostępne nieodpłatnie tylko dla repozytoriów publicznych. Dalej przedstawione zostaną trzy różne scenariusze publikacji serwisu wygenerowanego przez Hugo:\nwersja A: w serwisie GitHub upublicznione są pliki wejściowe wejściowe w formacie Markdown oraz wzorce stanowiące podstawę generacji stron HTML. GitHub Actions są wykorzystywane do wygenerowania i opublikowania nowej wersji serwisu. wersja B: w serwisie GitHub upublicznione są tylko pliki wynikowe, tj. zawartość wygenerowanego serwisu. GitHub Actions są wykorzystywane do jego publikacji. wersja C: repozytorium GitHub jest wykorzystywane tylko do przechowywania plików wejściowych generatora i nie musi być upublicznione. Generacja zawartości zawartości serwisu i jej upublicznienie wykorzystuje lokalne zasoby użytkownika. A. GitHub Actions Do publikacji serwisu zostanie wykorzystany automatyczne działanie, przy czym interfejs do jego konfiguracji znajduje się w menu Pages\nhttps://github.com/${gh_user}/${gh_repo}/settings/pages Jednak, aby publikacja się udała, w repozytorium musi istnieć gałąź Git o nazwie gh-pages. Można ją utworzyć w serwisie GitHub wybierając na stronie głównej repozytorium przycisk Branches i dodać wspomnianą gałąź.\nYour browser does not support the video tag. Dopiero teraz w menu Pages jako metodę publikacji należy wybrać GitHub Actions przy użyciu akcji Hugo.\nYour browser does not support the video tag. Takie działanie spowoduje powstanie w katalogu .github/workflows pliku hugo.yml o treści\nname: Deploy Hugo site to Pages on: push: branches: [\u0026quot;main\u0026quot;] workflow_dispatch: permissions: contents: read pages: write id-token: write concurrency: group: \u0026quot;pages\u0026quot; cancel-in-progress: false defaults: run: shell: bash jobs: # Build job build: runs-on: ubuntu-latest env: HUGO_VERSION: 0.114.0 steps: - name: Install Hugo CLI run: | wget -O ${{ runner.temp }}/hugo.deb https://github.com/gohugoio/hugo/releases/download/v${HUGO_VERSION}/hugo_extended_${HUGO_VERSION}_linux-amd64.deb \\ \u0026amp;\u0026amp; sudo dpkg -i ${{ runner.temp }}/hugo.deb - name: Install Dart Sass run: sudo snap install dart-sass - name: Checkout uses: actions/checkout@v3 with: submodules: recursive - name: Setup Pages id: pages uses: actions/configure-pages@v3 - name: Install Node.js dependencies run: \u0026quot;[[ -f package-lock.json || -f npm-shrinkwrap.json ]] \u0026amp;\u0026amp; npm ci || true\u0026quot; - name: Build with Hugo env: # For maximum backward compatibility with Hugo modules HUGO_ENVIRONMENT: production HUGO_ENV: production run: | hugo \\ --minify \\ --baseURL \u0026quot;${{ steps.pages.outputs.base_url }}/\u0026quot; - name: Upload artifact uses: actions/upload-pages-artifact@v2 with: path: ./public # Deployment job deploy: environment: name: github-pages url: ${{ steps.deployment.outputs.page_url }} runs-on: ubuntu-latest needs: build steps: - name: Deploy to GitHub Pages id: deployment uses: actions/deploy-pages@v2 Sekcja on: tego pliku definiuje jakie zdarzenia aktywują akcję. W zamieszczonym przykładzie skonfigurowano dwa takie zdarzenia: aktualizacja gałęzi main (opcja push:) oraz manualną aktywację (workflow_dispatch:) za pomocą interfejsu serwisu GitHub. Późniejsza zmiana metody publikacji zawartości repozytorium wymaga wcześniejszego całkowitego wyłączenia publikowania (unpublish). Biorąc pod uwagę, że skonfigurowana akcja zapewnia generację plików HTML, to synchronizacja plików z podkatalogu public nie jest konieczna.\nW tak skonfigurowanym repozytorium każdorazowa zmiana plików wejściowych powoduje zbudowanie serwisu. Takie postępowanie nie jest zbyt eleganckie, bowiem wiele z tych wywołań jest po prostu niepotrzebnych. Rozwiązaniem sytuacji jest utworzenie gałęzi devel i praca nad serwisem z jej wykorzystaniem. Do jej utworzenia można wykorzystać Code. Po wybraniu ikony synchronizacji wybieramy ... przy nazwie repozytorium, a następnie Branch-\u0026gt; Create Branch i wpisujemy devel. Teraz utworzoną gałąź publikujemy (...-\u0026gt;Branch-\u0026gt;Publish Branch), co zapewni, że zostanie ona przeniesiona do drzewa Git znajdującego się w repozytorium GitHub. Synchronizacja z repozytorium GitHub jest \u0026ldquo;bezkosztowa\u0026rdquo; gdyż zgodnie z konfiguracją tylko zmiany w gałęzi main włączają budowę serwisu.\nYour browser does not support the video tag. Dopiero gdy określony etap pracy zostanie osiągnięty, wtedy należy przełączyć się na gałąź main, włączyć do niej zmiany z gałęzi devel i zsynchronizować GitHub. Wtedy synchronizacja z serwisem GitHub spowoduje zbudowanie serwisu na mocy konfiguracji dyrektywy on:. Po wykonaniu synchronizacji aktywną gałąź w lokalnym repozytorium znowu należy przełączyć na devel.\nYour browser does not support the video tag. Zaletą przedstawionej konfiguracji jest możliwość aktualizacji zawartości serwisu z dowolnego miejsca za pomocą przeglądarki internetowej z wykorzystaniem natywnego interfejsu usługi GitHub lub https://vscode.dev/.\nB. GitHub Pages Może się zdarzyć, że jesteśmy zainteresowani tylko publikacją gotowego serwisu bez upubliczniania postaci źródłowej publikowanych stron. Idea takiej realizacji serwisu polega na utworzeniu w serwisie GitHub dwóch repozytoriów:\nprywatnego, nie korzystającego z GitHub Actions czy GitHub Pages i zawierającego tylko pliki źródłowe wykorzystywane w procesie generacji, publicznego. korzystającego z powyższych udogodnień i zawierającego tylko pliki wynikowe. Adaptacja poprzedniej konfiguracji do takiego wymogu jest stosunkowo prosta, należy jedynie pamiętać, że generator Hugo umieszcza pliki wynikowe w podkatalogu public:\nw usłudze GitHub repozytorium przechowujące pliki źródłowe przekształcamy w prywatne,\nw usłudze GitHub zakładamy nowe repozytorium, dalej będziemy się do niego odwoływać jako your_hugo_site,\nw lokalnym repozytorium czyścimy podkatalog public (znaczek * jest ważny):\nrm -rf public/* a następnie podpinamy do niego nowo utworzone repozytorium\ngit submodule add https://github.com/${gh-user}/your_hugo_site public/ Code zauważy zmianę i dopasuje swój interfejs tak aby umożliwić pracę z dwoma repozytoriami jednocześnie.\nPublikacja wymaga istnienia w repozytorium gałęzi gh-pages. Można ją utworzyć w serwisie GitHub wybierając na stronie głównej repozytorium przycisk Branches. Po wybraniu ikony synchronizacji wybieramy ... przy nazwie repozytorium , a następnie Branch-\u0026gt; Create Branch i wpisujemy gh-pages.\nYour browser does not support the video tag. Teraz w nowym repozytorium wystarczy aktywować mechanizm GitHub Pages.\nhttps://github.com/${gh_user}/${gh_repo}/settings/pages Tym razem należy wybrać działanie Static HTML (Build and deployment-\u0026gt;GitHub Actions). Taki wybór spowoduje, że w repozytorium your_hugo_site zostanie utworzone działanie .github/workflows/static.yml\nDomyślnie akcja publikacji dotyczy zawartości całego repozytorium, nie trzeba zatem nic zmieniać.\nname: Deploy static content to Pages on: push: branches: [\u0026quot;main\u0026quot;] workflow_dispatch: permissions: contents: read pages: write id-token: write concurrency: group: \u0026quot;pages\u0026quot; cancel-in-progress: false jobs: deploy: environment: name: github-pages url: ${{ steps.deployment.outputs.page_url }} runs-on: ubuntu-latest steps: - name: Checkout uses: actions/checkout@v3 - name: Setup Pages uses: actions/configure-pages@v3 - name: Upload artifact uses: actions/upload-pages-artifact@v2 with: # Upload only 'public/' folder path: '.' - name: Deploy to GitHub Pages id: deployment uses: actions/deploy-pages@v2 Praca w tak przygotowanym środowisku przebiega wg następującego cyklu:\nKorzystając z Code tworzymy lokalnie nową zawartość serwisu. Podgląd on-line pozwala na ocenę, czy nowo utworzone treści nadają się do publikacji. Gdy to jest konieczne wykonujemy synchronizację z repozytorium prywatnym.\nGdy tak jest, w gotowych do publikacji materiałach ustawiamy draft: false i generujemy nową postać serwisu komendą\nhugo --minify Takie działanie spowoduje aktualizację zawartości katalogu public/ i konieczność synchronizacji ze zdalnym repozytorium. Po jej wykonaniu skonfigurowana akcja GitHub automatycznie opublikuje nową postać serwisu. Należy jedynie pamiętać, że wykonywanie akcji może nie być natychmiastowe i może chwilę potrwać.\nW przedstawionej konfiguracji można zachować prywatność danych wejściowych. Ceną jest komplikacja schematu pracy nad serwisem.\nC. Własny hosting serwisu W niniejszym punkcie opisano działania niezbędne do lokalnego hostingu tworzonego serwisu. Do jego realizacji potrzebna będzie maszyna z publicznym (routowalnym) adresem IP. Nie ma wymogu aby adres ten był stały. Maszyna musi mieć skonfigurowany dostęp za pomocą protokołu SSH. Do hostingu wykorzystane zostaną kontenery Docker, zatem również konieczna jest instalacja tego serwera na maszynie publikującej serwis WWW. Nic nie stoi na przeszkodzie, aby maszyna wykorzystana do publikacji była zrealizowana jako wirtualny host w chmurze. Serwis WWW musi mieć swoją nazwę, zatem należy wykupić domenę lub skorzystać z darmowych odmian dynamicznych serwisów DNS do skojarzenia nazwy serwisu z adresem IP maszyny udostępniającej serwis. W prezentowanym dalej przykładzie zostanie wykorzystana usługa DuckDNS.\nDo realizacji lokalnego hostingu jakiekolwiek serwisu niezbędny jest routowalny (publiczny) adres IP. W omawianej konfiguracji przyjęto założenie, że adres ten jest przypisany do routera z usługą NAT, natomiast host realizujący serwis znajduje się w sieci lokalnej \u0026ldquo;schowanej\u0026rdquo; za routerem i korzystającej nieroutowalnych (prywatnych) adresów IP. Przyjęto również założenie, że host, na którym rozwijany jest serwis i na którym zainstalowano Code, znajduje się w tej samej sieci co host z serwisem WWW. Na hoście z serwisem WWW działa serwer SSH.\nDo publicznego adresu IP musi być przypisana kwalifikowana domenowa nazwa hosta (FQDN) w formacie nazwa.serwisu.moja.domena.org. Jeżeli adres IP jest stały, to wystarczy wykupić dla niego domenę. Dla adresów zmiennych konieczne jest skorzystanie z usługi dynamicznego DNS. Idea działania takiej usługi polega na tym, że host o zmiennym adresie IP otrzymuje identyfikator unikalny w ramach dynamicznego serwisu DNS i co pewien czas \u0026ldquo;melduje\u0026rdquo; swój adres IP. Usługa dynamicznego DNS rozpowszechnia tę informację, komunikując się z innymi serwisami DNS, które aktualizują swoją konfigurację. Oczywiście, dla stałych adresów IP skorzystanie z usługi dynamicznego DNS do uzyskania nazwy domenowej jest również możliwe. Darmowe usługi dynamicznego DNS pozwalają zazwyczaj tylko na określenie nazwy hosta, a domenowa część nazwy jest zadana przez dostawcę serwisu.\nZaproponowana konfiguracja hosta realizującego serwis pozwala na prowadzenie wielu serwisów WWW na tym samym adresie IP, jednak o różnych nazwach domenowych. Oczywiście nic nie stoi na przeszkodzie, aby do tego samego numeru IP przypisać wiele nazw domenowych. Dalej do skojarzenia adresu IP z nazwą domenową wykorzystany darmowy serwis https://duckdns.org. W serwisie należy założyć konto. W ramach każdego konta można nieodpłatnie założyć kilka domen (wraz z subdomenami). Każdy użytkownik serwisu ma przypisany unikalny token, który jest wykorzystywany do aktualizacji zmiennego adresu IP.\nNa hoście, który ma utrzymywać serwis www konieczna jest instalacja serwera Dockera. Użytkownik zdalnego hosta powinien być dodany do grupy docker, co uwolnia od konieczności pracy z Dockerem na prawach administratora.\nsudo usermod -aG docker ${USER} Aby powyższa zmiana odniosła skutek, należy się wylogować i zalogować lub w otwartych sesjach wydać komendę\nnewgrp docker W zdalnym zarządzaniu kontenerami dobrze spisuje się Portainer. Uboższa jego wersja jest dostępna nieodpłatnie. Instalacja sprowadza się do dwóch komend\ndocker volume create portainer_data docker run -d -p 8000:8000 -p 9000:9000 --name portainer --restart=always -v /var/run/docker.sock:/var/run/docker.sock -v portainer_data:/data portainer/portainer-ce:latest Od teraz zainstalowanymi obrazami i kontenerami można zarządzać łącząc się przeglądarką przy użyciu protokołu HTTP z portem 9000 hosta z Portainerem.\nInstalacja własnego serwisu WWW jest bardzo prosta. Do katalogu services hosta\nmkdir -p /home/your_host_account/services/ cd /home/your_host_account/services/ wgrywamy plik docker-compose.yml o następującej treści\nversion: \u0026quot;3\u0026quot; networks: frontend: name: dmz services: duckdns: image: lscr.io/linuxserver/duckdns:latest container_name: dmz-duckdns environment: - PUID=1000 #optional - PGID=1000 #optional - TZ=Europe/Warsaw - SUBDOMAINS=misiek,kociak - TOKEN=xxxxxxxx-yyyy-zzzz-aaaa-bbbbbbbbbbbb - LOG_FILE=true #optional volumes: - /home/your_host_account/services/duckdns:/config #optional networks: - frontend restart: unless-stopped nginxproxy: image: 'jc21/nginx-proxy-manager:latest' container_name: dmz-nginxproxy restart: unless-stopped ports: - '8888:80' - '81:81' - '4443:443' volumes: - /home/your_host_account/services/npm/data:/data - /home/your_host_account/services/npm/letsencrypt:/etc/letsencrypt networks: - frontend www0: image: lscr.io/linuxserver/nginx:latest container_name: dmz-www0 environment: - PUID=1000 - PGID=1000 - TZ=Europe/London volumes: - /home/your_host_account/services/www0:/config restart: unless-stopped networks: - frontend www1: image: lscr.io/linuxserver/nginx:latest container_name: dmz-www1 environment: - PUID=1000 - PGID=1000 - TZ=Europe/London volumes: - /home/your_host_account/services/www1:/config restart: unless-stopped networks: - frontend Następnie należy utworzyć puste katalogi, w których będą przechowywane dane z kontenerów\nmkdir -p {duckdns,npm/data,npm/letsencrypt,www0,www1} Ostateczne uruchomienie stosu usług realizuje komenda\ndocker compose up -d wydana w katalogu z plikiem docker-compose.yml.\nKontener duckdns odpowiada za aktualizację adresu IP serwisu w usłudze dynamicznego DNS. Kontener nginx-proxy-manager realizuje reverse-proxy, za którym schowane są serwisy www0 i www1. Kontener ten również troszczy się o utrzymanie aktualnego certyfikatu dla domen skonfigurowanych w ramach proxy. Zarządzanie proxy (i domenami) realizuje się za pomocą interfejsu WWW dostępnego na porcie 81. Interfejsy http i https zarządzanych usług dostępne są odpowiednio na portach 8888 i 4443 dlatego na routerze realizującym NAT należy uaktywnić Port Forwarding i zrealizować przypisanie \u0026ldquo;80-\u0026gt;ip_host:8888\u0026rdquo;, \u0026ldquo;443-\u0026gt;ip_host:4443\u0026rdquo;. W ten sposób ruch docierający do routera i skojarzony z protokołami http i https zostanie przekierowany do usługi proxy, a ta, na podstawie nazwy domenowej, podejmie decyzję do którego serwisu WWW ruch powinien być skierowany. Wspomniane przekierowanie portów w routerze musi być wykonane przed konfiguracją proxy, bowiem są one wykorzystywane do komunikacji z serwisem LetsEncrypt w celu uzyskania certyfikatów SSL.\nKonfiguracja Nginx Proxy Manager jest stosunkowo prosta. Pierwsze łączenie z interfejsem administracyjnym (port 81) wymaga zmiany hasła i nazwy użytkownika (domyślnymi wartościami są admin i changeme). Konfigurację rozpoczynamy od uzyskania certyfikatów dla skonfigurowanych domen (Dashboard-\u0026gt;SSL Certificates). Następnie dodajemy Proxy Hosts. Jako nazwę Forward Hostname należy podać nazwę kontenera,czyli w rozważanym przypadku dmz-www0 lub dmz-www1.\nŚrodowisko do prezentacji zawartości serwisu jest już przygotowane. Należy jedynie pamiętać, aby w konfiguracji Hugo (config.toml) również ustawić nazwę skonfigurowaną w DuckDNS. Serwis generuje się jak zwykle, zatrzymując serwer i wydając w katalogu projektu komendę\nhugo Wystarczy teraz przesłać zawartość katalogu public/ do katalogu głównego serwera WWW tj.\nscp -r public/* user@ip_host_lan:/home/your_host_account/services/www1/www/ i serwis jest opublikowany.\nPodsumowanie Przedstawione scenariusze wykorzystania Code do pracy nad serwisem WWW nie wyczerpują wszystkich możliwości. Bezpośrednia praca nad plikami zapisanymi w zdalnym repozytorium GitHub jest możliwa tylko przy użyciu przeglądarki. Interfejs imitujący Code i zapewniający nieco zubożoną funkcjonalność jest dostepny pod adresem https://vscode.dev. Jedną z takich brakujących funkcjonalności jest praca we własnym kontenerze. Jednak nie oznacza to, że jest to niemożliwe. Wystarczy sobie uświadomić, że kontenery Docker\u0026lsquo;a traktowane są jak zdalne maszyny. W ramach usługi GitHub można do tego wykorzystać funkcję Codespaces dostępną w głównym menu użytkownika. Usługa to pozwala na utworzenie w chmurze wirtualnych maszyn na których uruchomione jest to samo oprogramowanie które umożliwiają komunikację z kontenerem. Jako interfejs użytkownika jest wykorzystywany https://vscode.dev, co oznacza, że do pracy z taką zdalną maszyną wystarczy przeglądarka. Należy jednak pamiętać, że Codespaces nie są całkowicie bezpłatne. Istnieją limity zasobów, które można w zadanym okresie czasu wykorzystać za darmo. Potrzeby powyżej limitów określonych przez dostawcę usługi wymagają wniesienia opłat.\n","date":"2023-08-05","permalink":"https://pejotzet.github.io/pejotzet/post/2023-08-05-static-site-in-hugo/","tags":["Code","DevContainer","Hugo","Github Actions"],"title":"Jak założyć i utrzymywać statyczny serwis WWW wykorzystując Docker'a i Code"}]